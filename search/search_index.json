{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"sbi \u00b6 sbi : A Python toolbox for simulation-based inference. Inference can be run in a single line of code: posterior = infer('SNPE', prior, simulator, num_simulations=1000) To see applications of simulation-based inference to canonical problems in neuroscience, read our preprint: Training deep neural density estimators to identify mechanistic models of neural dynamics . To learn more about the general motivation behind simulation-based inference, and inference methods included in sbi , keep on reading. Motivation and approach \u00b6 Many areas of science and engineering make extensive use of complex, stochastic, numerical simulations to describe the structure and dynamics of the processes being investigated. A key challenge in simulation-based science is linking simulation models to empirical data: Bayesian inference provides a general and powerful framework for identifying the set of parameters which are consistent both with empirical data and prior knowledge. One of the key quantities required for statistical inference, the likelihood of observed data given parameters, \\(\\mathcal{L}(\\theta) = p(x_o|\\theta)\\) , is typically intractable for simulation-based models, rendering conventional statistical approaches inapplicable. sbi implements three powerful machine-learning methods to address this problem: Sequential Neural Posterior Estimation (SNPE) Sequential Neural Likelihood Estimation (SNLE) Sequential Neural Ratio Estimation (SNRE) Depending on the problem, the different methods can be more or less effective. Goal: Algorithmically identify mechanistic models which are consistent with data. Each of the above-described methods has three inputs: A candidate mechanistic model, prior knowledge or constraints on model parameters, and data (or summary statistics). The methods then proceed by: sampling parameters from the prior and simulating synthetic datasets from these parameters, and using a deep density estimation neural network to learn the (probabilistic) association between data (or data features) and underlying parameters, i.e. to learn statistical inference from simulated data. They way in which this association is learned differs between the above methods. This learned neural network is then applied to empirical data to derive the full space of parameters consistent with the data and the prior, i.e. the posterior distribution. High posterior probability is assigned to parameters which are consistent with both the data and the prior, low probability to inconsistent parameters. While SNPE directly learns the posterior distribution, SNLE and SNRE need an extra MCMC step to infer the posterior. If needed, an initial estimate of the posterior can be used to adaptively generate additional informative simulations. Publications \u00b6 Refer to the following papers for additional details on the inference methods included in sbi : SNPE \u00b6 Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios & Murray (NeurIPS 2016) [PDF] [BibTeX] Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) [PDF] [BibTeX] Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher & Macke (ICML 2019) [PDF] [BibTeX] SNLE \u00b6 Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows by Papamakarios, Sterratt & Murray (AISTATS 2019) [PDF] [BibTeX] SNRE \u00b6 Likelihood-free MCMC with Amortized Approximate Likelihood Ratios by Hermans, Begy & Louppe (ICML 2020) [PDF] On Contrastive Learning for Likelihood-free Inference Durkan, Murray & Papamakarios (ICML 2020) [PDF] . See Cranmer, Brehmer, Louppe (2019) for a recent review on simulation-based inference and our recent preprint Training deep neural density estimators to identify mechanistic models of neural dynamics (Goncalves et al., 2019) for applications to canonical problems in neuroscience.","title":"Home"},{"location":"#sbi","text":"sbi : A Python toolbox for simulation-based inference. Inference can be run in a single line of code: posterior = infer('SNPE', prior, simulator, num_simulations=1000) To see applications of simulation-based inference to canonical problems in neuroscience, read our preprint: Training deep neural density estimators to identify mechanistic models of neural dynamics . To learn more about the general motivation behind simulation-based inference, and inference methods included in sbi , keep on reading.","title":"sbi"},{"location":"#motivation-and-approach","text":"Many areas of science and engineering make extensive use of complex, stochastic, numerical simulations to describe the structure and dynamics of the processes being investigated. A key challenge in simulation-based science is linking simulation models to empirical data: Bayesian inference provides a general and powerful framework for identifying the set of parameters which are consistent both with empirical data and prior knowledge. One of the key quantities required for statistical inference, the likelihood of observed data given parameters, \\(\\mathcal{L}(\\theta) = p(x_o|\\theta)\\) , is typically intractable for simulation-based models, rendering conventional statistical approaches inapplicable. sbi implements three powerful machine-learning methods to address this problem: Sequential Neural Posterior Estimation (SNPE) Sequential Neural Likelihood Estimation (SNLE) Sequential Neural Ratio Estimation (SNRE) Depending on the problem, the different methods can be more or less effective. Goal: Algorithmically identify mechanistic models which are consistent with data. Each of the above-described methods has three inputs: A candidate mechanistic model, prior knowledge or constraints on model parameters, and data (or summary statistics). The methods then proceed by: sampling parameters from the prior and simulating synthetic datasets from these parameters, and using a deep density estimation neural network to learn the (probabilistic) association between data (or data features) and underlying parameters, i.e. to learn statistical inference from simulated data. They way in which this association is learned differs between the above methods. This learned neural network is then applied to empirical data to derive the full space of parameters consistent with the data and the prior, i.e. the posterior distribution. High posterior probability is assigned to parameters which are consistent with both the data and the prior, low probability to inconsistent parameters. While SNPE directly learns the posterior distribution, SNLE and SNRE need an extra MCMC step to infer the posterior. If needed, an initial estimate of the posterior can be used to adaptively generate additional informative simulations.","title":"Motivation and approach"},{"location":"#publications","text":"Refer to the following papers for additional details on the inference methods included in sbi :","title":"Publications"},{"location":"#snpe","text":"Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios & Murray (NeurIPS 2016) [PDF] [BibTeX] Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) [PDF] [BibTeX] Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher & Macke (ICML 2019) [PDF] [BibTeX]","title":"SNPE"},{"location":"#snle","text":"Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows by Papamakarios, Sterratt & Murray (AISTATS 2019) [PDF] [BibTeX]","title":"SNLE"},{"location":"#snre","text":"Likelihood-free MCMC with Amortized Approximate Likelihood Ratios by Hermans, Begy & Louppe (ICML 2020) [PDF] On Contrastive Learning for Likelihood-free Inference Durkan, Murray & Papamakarios (ICML 2020) [PDF] . See Cranmer, Brehmer, Louppe (2019) for a recent review on simulation-based inference and our recent preprint Training deep neural density estimators to identify mechanistic models of neural dynamics (Goncalves et al., 2019) for applications to canonical problems in neuroscience.","title":"SNRE"},{"location":"contribute/","text":"User experiences, bugs, and feature requests \u00b6 If you are using sbi to infer the parameters of a simulators, we would be delighted to know how it worked for you. If it didn\u2019t work according to plan, please open up an issue and tell us more about your use case: the dimensionality of the input parameters, the type of simulator and the dimensionality of the output. To report bugs and suggest features (including better documentation), please equally head over to issues on GitHub . Code contributions: pull requests. \u00b6 In general, we use pull requests to make changes to sbi . Setting up a development environment \u00b6 Clone the repo and install all the dependencies using the environment.yml file to create a conda environment: conda env create -f environment.yml . If you already have an sbi environment and want to refresh dependencies, just run conda env update -f environment.yml --prune . Alternatively, you can install via setup.py using pip install -e \".[dev]\" (the dev flag installs development and testing dependencies). Code style \u00b6 For docstrings and comments, we use Google Style . Code needs to pass through the following tools, which are installed along with sbi : black : Automatic code formatting for Python. You can run black manually from the console using black . in the top directory of the repository, which will format all files. isort : Used to consistently order imports. You can run isort manually from the console using isort -y in the top directory. Documentation \u00b6 The documentation is entirely written in markdown ( basic markdown guide ). It would be of great help, if you fixed mistakes, and edited where things are unclear. After a PR with documentation changes has been merged, the online documentation will be updated automatically in a couple of minutes. If you want to test a local build of the documentation, take a look at docs/README.md . Examples are collected in notebooks in examples/ . Binary files and Jupyter notebooks \u00b6 Using sbi We use git lfs to store large binary files. Those files are not downloaded by cloning the repository, but you have to pull them separately. To do so follow installation instructions here https://git-lfs.github.com/ . In particular, in a freshly cloned repository on a new machine, you will need to run both git-lfs install and git-lfs pull . Contributing to sbi We use a filename filter to track lfs files. Once you installed and pulled git lfs you can add a file to git lfs by appending _gitlfs to the basename, e.g., oldbase_gitlfs.npy . Then add the file to the index, commit, and it will be tracked by git lfs. Additionally, to avoid large diffs due to Jupyter notebook outputs we are using nbstripout to remove output from notebooks before every commit. The nbstripout package is downloaded automatically during installation of sbi . However, please make sure to set up the filter yourself , e.g., through nbstriout --install or with different options as described here .","title":"Contribute"},{"location":"contribute/#user-experiences-bugs-and-feature-requests","text":"If you are using sbi to infer the parameters of a simulators, we would be delighted to know how it worked for you. If it didn\u2019t work according to plan, please open up an issue and tell us more about your use case: the dimensionality of the input parameters, the type of simulator and the dimensionality of the output. To report bugs and suggest features (including better documentation), please equally head over to issues on GitHub .","title":"User experiences, bugs, and feature requests"},{"location":"contribute/#code-contributions-pull-requests","text":"In general, we use pull requests to make changes to sbi .","title":"Code contributions: pull requests."},{"location":"contribute/#setting-up-a-development-environment","text":"Clone the repo and install all the dependencies using the environment.yml file to create a conda environment: conda env create -f environment.yml . If you already have an sbi environment and want to refresh dependencies, just run conda env update -f environment.yml --prune . Alternatively, you can install via setup.py using pip install -e \".[dev]\" (the dev flag installs development and testing dependencies).","title":"Setting up a development environment"},{"location":"contribute/#code-style","text":"For docstrings and comments, we use Google Style . Code needs to pass through the following tools, which are installed along with sbi : black : Automatic code formatting for Python. You can run black manually from the console using black . in the top directory of the repository, which will format all files. isort : Used to consistently order imports. You can run isort manually from the console using isort -y in the top directory.","title":"Code style"},{"location":"contribute/#documentation","text":"The documentation is entirely written in markdown ( basic markdown guide ). It would be of great help, if you fixed mistakes, and edited where things are unclear. After a PR with documentation changes has been merged, the online documentation will be updated automatically in a couple of minutes. If you want to test a local build of the documentation, take a look at docs/README.md . Examples are collected in notebooks in examples/ .","title":"Documentation"},{"location":"contribute/#binary-files-and-jupyter-notebooks","text":"","title":"Binary files and Jupyter notebooks"},{"location":"credits/","text":"Credits \u00b6 sbi is licensed under the Affero General Public License version 3 (AGPLv3) and Copyright (C) 2020 \u00c1lvaro Tejero-Cantero, Jakob H. Macke, Jan-Matthis L\u00fcckmann, Michael Deistler, Jan F. B\u00f6lts. Copyright (C) 2020 Conor M. Durkan. Important dependencies and prior art \u00b6 sbi uses PyTorch and tries to align with the interfaces (e.g. for probability distributions) adopted by PyTorch . sbi uses density estimators from bayesiains/nflows by Conor M.Durkan , George Papamakarios and Artur Bekasov . These are proxied through pyknos , a package focused on density estimation. sbi started as a fork of conormdurkan/lfi , by Conor M.Durkan . See README.md for a list of methods implemented in sbi .","title":"Credits"},{"location":"credits/#credits","text":"sbi is licensed under the Affero General Public License version 3 (AGPLv3) and Copyright (C) 2020 \u00c1lvaro Tejero-Cantero, Jakob H. Macke, Jan-Matthis L\u00fcckmann, Michael Deistler, Jan F. B\u00f6lts. Copyright (C) 2020 Conor M. Durkan.","title":"Credits"},{"location":"credits/#important-dependencies-and-prior-art","text":"sbi uses PyTorch and tries to align with the interfaces (e.g. for probability distributions) adopted by PyTorch . sbi uses density estimators from bayesiains/nflows by Conor M.Durkan , George Papamakarios and Artur Bekasov . These are proxied through pyknos , a package focused on density estimation. sbi started as a fork of conormdurkan/lfi , by Conor M.Durkan . See README.md for a list of methods implemented in sbi .","title":"Important dependencies and prior art"},{"location":"install/","text":"Installation \u00b6 Quick start \u00b6 Clone the repo and install all the dependencies using the environment.yml file to create a conda environment: conda env create -f environment.yml . If you already have an sbi environment and want to refresh dependencies, just run conda env update -f environment.yml --prune . Alternatively, you can install via setup.py using pip install -e \".[dev]\" (the dev flag installs development and testing dependencies). git clone https://github.com/mackelab/sbi.git cd sbi pip install -e \".dev\"","title":"Installation"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#quick-start","text":"Clone the repo and install all the dependencies using the environment.yml file to create a conda environment: conda env create -f environment.yml . If you already have an sbi environment and want to refresh dependencies, just run conda env update -f environment.yml --prune . Alternatively, you can install via setup.py using pip install -e \".[dev]\" (the dev flag installs development and testing dependencies). git clone https://github.com/mackelab/sbi.git cd sbi pip install -e \".dev\"","title":"Quick start"},{"location":"reference/","text":"API Reference \u00b6 Inference \u00b6 sbi.inference.snpe.snpe_a.SNPE_A \u00b6 __init__ ( self , simulator , prior , x_shape = None , num_workers = 1 , simulation_batch_size = 1 , density_estimator = 'mdn' , calibration_kernel = None , z_score_x = True , z_score_min_std = 1e-07 , exclude_invalid_x = True , device = device ( type = 'cpu' ), logging_level = 'WARNING' , summary_writer = None , show_progress_bars = True , show_round_summary = False ) special SNPE-A [1]. CURRENTLY NOT IMPLEMENTED. [1] Fast epsilon-free Inference of Simulation Models with Bayesian Conditional Density Estimation , Papamakarios et al., NeurIPS 2016, https://arxiv.org/abs/1605.06376 . Source code in sbi/inference/snpe/snpe_a.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , simulator : Callable , prior , x_shape : Optional [ torch . Size ] = None , num_workers : int = 1 , simulation_batch_size : int = 1 , density_estimator : Union [ str , nn . Module ] = \"mdn\" , calibration_kernel : Optional [ Callable ] = None , z_score_x : bool = True , z_score_min_std : float = 1e-7 , exclude_invalid_x : bool = True , device : Union [ torch . device , str ] = get_default_device (), logging_level : Union [ int , str ] = \"WARNING\" , summary_writer : Optional [ SummaryWriter ] = None , show_progress_bars : bool = True , show_round_summary : bool = False , ): \"\"\"SNPE-A [1]. CURRENTLY NOT IMPLEMENTED. [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional Density Estimation_, Papamakarios et al., NeurIPS 2016, https://arxiv.org/abs/1605.06376. \"\"\" raise NotImplementedError sbi.inference.snpe.snpe_b.SNPE_B \u00b6 __init__ ( self , simulator , prior , x_shape = None , num_workers = 1 , simulation_batch_size = 1 , density_estimator = 'mdn' , z_score_x = True , z_score_min_std = 1e-07 , calibration_kernel = None , retrain_from_scratch_each_round = False , discard_prior_samples = False , exclude_invalid_x = True , device = device ( type = 'cpu' ), logging_level = 'WARNING' , summary_writer = None , show_progress_bars = True , show_round_summary = False ) special SNPE-B [1]. CURRENTLY NOT IMPLEMENTED. [1] Flexible statistical inference for mechanistic models of neural dynamics , Lueckmann, Gon\u00e7alves et al., NeurIPS 2017, https://arxiv.org/abs/1711.01861 . See docstring of PosteriorEstimator class for all other arguments. Source code in sbi/inference/snpe/snpe_b.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def __init__ ( self , simulator : Callable , prior , x_shape : Optional [ torch . Size ] = None , num_workers : int = 1 , simulation_batch_size : Optional [ int ] = 1 , density_estimator : Union [ str , nn . Module ] = \"mdn\" , z_score_x : bool = True , z_score_min_std : float = 1e-7 , calibration_kernel : Optional [ Callable ] = None , retrain_from_scratch_each_round : bool = False , discard_prior_samples : bool = False , exclude_invalid_x : bool = True , device : Union [ torch . device , str ] = get_default_device (), logging_level : Union [ int , str ] = \"WARNING\" , summary_writer : Optional [ SummaryWriter ] = None , show_progress_bars : bool = True , show_round_summary : bool = False , ): r \"\"\"SNPE-B [1]. CURRENTLY NOT IMPLEMENTED. [1] _Flexible statistical inference for mechanistic models of neural dynamics_, Lueckmann, Gon\u00e7alves et al., NeurIPS 2017, https://arxiv.org/abs/1711.01861. See docstring of `PosteriorEstimator` class for all other arguments. \"\"\" raise NotImplementedError ( \"SNPE-B is not yet implemented in the sbi package, see issue #199.\" ) super () . __init__ ( simulator = simulator , prior = prior , x_shape = x_shape , num_workers = num_workers , simulation_batch_size = simulation_batch_size , density_estimator = density_estimator , z_score_x = z_score_x , z_score_min_std = z_score_min_std , calibration_kernel = calibration_kernel , retrain_from_scratch_each_round = retrain_from_scratch_each_round , discard_prior_samples = discard_prior_samples , exclude_invalid_x = exclude_invalid_x , device = device , logging_level = logging_level , show_progress_bars = show_progress_bars , show_round_summary = show_round_summary , ) sbi.inference.snpe.snpe_c.SNPE_C \u00b6 __call__ ( self , num_rounds , num_simulations_per_round , x_o = None , num_atoms = 10 , batch_size = 50 , learning_rate = 0.0005 , validation_fraction = 0.1 , stop_after_epochs = 20 , max_num_epochs = None , clip_max_norm = 5.0 , calibration_kernel = None , exclude_invalid_x = True , z_score_x = True , z_score_min_std = 1e-07 , discard_prior_samples = False , retrain_from_scratch_each_round = False ) special Parameters: Name Type Description Default num_atoms int Number of atoms to use for classification. 10 Returns: Type Description NeuralPosterior Source code in sbi/inference/snpe/snpe_c.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def __call__ ( self , num_rounds : int , num_simulations_per_round : OneOrMore [ int ], x_o : Optional [ Tensor ] = None , num_atoms : int = 10 , batch_size : int = 50 , learning_rate : float = 5e-4 , validation_fraction : float = 0.1 , stop_after_epochs : int = 20 , max_num_epochs : Optional [ int ] = None , clip_max_norm : Optional [ float ] = 5.0 , calibration_kernel : Optional [ Callable ] = None , exclude_invalid_x : bool = True , z_score_x : bool = True , z_score_min_std : float = 1e-7 , discard_prior_samples : bool = False , retrain_from_scratch_each_round : bool = False , ) -> NeuralPosterior : \"\"\" Args: num_atoms: Number of atoms to use for classification. Returns: \"\"\" # WARNING: sneaky trick ahead. We proxy the parent's `__call__` here, # requiring the signature to have `num_atoms`, save it for use below, and # continue. It's sneaky because we are using the object (self) as a namespace # to pass arguments between functions, and that's implicit state management. self . _num_atoms = num_atoms kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" , \"num_atoms\" )) return super () . __call__ ( ** kwargs ) __init__ ( self , simulator , prior , x_shape = None , num_workers = 1 , simulation_batch_size = 1 , density_estimator = 'maf' , sample_with_mcmc = False , mcmc_method = 'slice_np' , use_combined_loss = False , device = device ( type = 'cpu' ), logging_level = 'WARNING' , summary_writer = None , show_progress_bars = True , show_round_summary = False ) special SNPE-C / APT [1]. [1] Automatic Posterior Transformation for Likelihood-free Inference , Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488 . Parameters: Name Type Description Default use_combined_loss bool Whether to train the neural net also on prior samples using maximum likelihood in addition to training it on all samples using atomic loss. The extra MLE loss helps prevent density leaking with bounded priors. False See docstring of PosteriorEstimator class for all other arguments. Source code in sbi/inference/snpe/snpe_c.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , simulator : Callable , prior , x_shape : Optional [ torch . Size ] = None , num_workers : int = 1 , simulation_batch_size : int = 1 , density_estimator : Union [ str , nn . Module ] = \"maf\" , sample_with_mcmc : bool = False , mcmc_method : str = \"slice_np\" , use_combined_loss : bool = False , device : Union [ torch . device , str ] = get_default_device (), logging_level : Union [ int , str ] = \"WARNING\" , summary_writer : Optional [ SummaryWriter ] = None , show_progress_bars : bool = True , show_round_summary : bool = False , ): r \"\"\"SNPE-C / APT [1]. [1] _Automatic Posterior Transformation for Likelihood-free Inference_, Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488. Args: use_combined_loss: Whether to train the neural net also on prior samples using maximum likelihood in addition to training it on all samples using atomic loss. The extra MLE loss helps prevent density leaking with bounded priors. See docstring of `PosteriorEstimator` class for all other arguments. \"\"\" self . _use_combined_loss = use_combined_loss super () . __init__ ( simulator = simulator , prior = prior , x_shape = x_shape , num_workers = num_workers , simulation_batch_size = simulation_batch_size , density_estimator = density_estimator , sample_with_mcmc = sample_with_mcmc , mcmc_method = mcmc_method , device = device , logging_level = logging_level , summary_writer = summary_writer , show_progress_bars = show_progress_bars , show_round_summary = show_round_summary , ) sbi.inference.snle.snle_a.SNLE_A \u00b6 SNL=SNLE_A is the only likelihood-based estimator as of now. sbi.inference.snre.snre_a.SNRE_A \u00b6 AALR[1], here known as SNRE_A. [1] _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_, Hermans et al., ICML 2020, https://arxiv.org/abs/1903.04057 sbi.inference.snre.snre_b.SNRE_B \u00b6 Models \u00b6 sbi.utils.get_nn_models.posterior_nn ( model , prior , x_o_shape , embedding = Identity (), hidden_features = 50 , mdn_num_components = 20 , made_num_mixture_components = 10 , made_num_blocks = 4 , flow_num_transforms = 5 ) \u00b6 Neural posterior density estimator Parameters: Name Type Description Default model str Model, one of maf / mdn / made / nsf required prior Prior distribution. required x_o_shape torch.Size Shape of a single observation. Used as input size to the NN. required embedding nn.Module Embedding network Identity() hidden_features int For all, number of hidden features 50 mdn_num_components int For MDNs only, number of components 20 made_num_mixture_components int For MADEs only, number of mixture components 10 made_num_blocks int For MADEs only, number of blocks 4 flow_num_transforms int For flows only, number of transforms 5 Returns: Type Description nn.Module Neural network Source code in sbi/utils/get_nn_models.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def posterior_nn ( model : str , prior , x_o_shape : torch . Size , embedding : nn . Module = nn . Identity (), hidden_features : int = 50 , mdn_num_components : int = 20 , made_num_mixture_components : int = 10 , made_num_blocks : int = 4 , flow_num_transforms : int = 5 , ) -> nn . Module : \"\"\"Neural posterior density estimator Args: model: Model, one of maf / mdn / made / nsf prior: Prior distribution. x_o_shape: Shape of a single observation. Used as input size to the NN. embedding: Embedding network hidden_features: For all, number of hidden features mdn_num_components: For MDNs only, number of components made_num_mixture_components: For MADEs only, number of mixture components made_num_blocks: For MADEs only, number of blocks flow_num_transforms: For flows only, number of transforms Returns: Neural network \"\"\" # We need these asserts because mean and std can be defined outside, prior to user # input checks. prior_mean = prior . mean prior_std = prior . stddev assert ( prior_mean . dtype == float32 ), f \"Prior mean must have dtype float32, is { prior_mean . dtype } .\" assert ( prior_std . dtype == float32 ), f \"Prior std must have dtype float32, is { prior_std . dtype } .\" standardizing_transform = transforms . AffineTransform ( shift =- prior_mean / prior_std , scale = 1 / prior_std ) theta_numel = prior_mean . numel () x_o_numel = x_o_shape . numel () if theta_numel == 1 : _check_1d_flow_limitations ( model , \"parameter\" ) if model == \"mdn\" : neural_net = MultivariateGaussianMDN ( features = theta_numel , context_features = x_o_numel , hidden_features = hidden_features , hidden_net = nn . Sequential ( nn . Linear ( x_o_numel , hidden_features ), nn . ReLU (), nn . Dropout ( p = 0.0 ), nn . Linear ( hidden_features , hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . ReLU (), ), num_components = mdn_num_components , custom_initialization = True , ) elif model == \"made\" : transform = standardizing_transform distribution = distributions_ . MADEMoG ( features = theta_numel , hidden_features = hidden_features , context_features = x_o_numel , num_blocks = made_num_blocks , num_mixture_components = made_num_mixture_components , use_residual_blocks = True , random_mask = False , activation = relu , dropout_probability = 0.0 , use_batch_norm = False , custom_initialization = True , ) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"maf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . MaskedAffineAutoregressiveTransform ( features = theta_numel , hidden_features = hidden_features , context_features = x_o_numel , num_blocks = 2 , use_residual_blocks = False , random_mask = False , activation = tanh , dropout_probability = 0.0 , use_batch_norm = True , ), transforms . RandomPermutation ( features = theta_numel ), ] ) for _ in range ( flow_num_transforms ) ] ) transform = transforms . CompositeTransform ([ standardizing_transform , transform ,]) distribution = distributions_ . StandardNormal (( theta_numel ,)) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"nsf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . PiecewiseRationalQuadraticCouplingTransform ( mask = create_alternating_binary_mask ( features = theta_numel , even = ( i % 2 == 0 ) ), transform_net_create_fn = lambda in_features , out_features : nets . ResidualNet ( in_features = in_features , out_features = out_features , hidden_features = hidden_features , context_features = x_o_numel , num_blocks = 2 , activation = relu , dropout_probability = 0.0 , use_batch_norm = False , ), num_bins = 10 , tails = \"linear\" , tail_bound = 3.0 , apply_unconditional_transform = False , ), transforms . LULinear ( theta_numel , identity_init = True ), ] ) for i in range ( flow_num_transforms ) ] ) transform = transforms . CompositeTransform ([ standardizing_transform , transform ,]) distribution = distributions_ . StandardNormal (( theta_numel ,)) neural_net = flows . Flow ( transform , distribution , embedding ) else : raise ValueError return neural_net sbi.utils.get_nn_models.likelihood_nn ( model , theta_shape , x_o_shape , embedding = None , hidden_features = 50 , mdn_num_components = 20 , made_num_mixture_components = 10 , made_num_blocks = 4 , flow_num_transforms = 5 ) \u00b6 Neural likelihood density estimator Parameters: Name Type Description Default model str Model, one of maf / mdn / made / nsf required theta_numel event shape of the prior, number of parameters. required x_o_numel number of elements in a single data point. required embedding Optional[nn.Module] Embedding network None hidden_features int For all, number of hidden features 50 mdn_num_components int For MDNs only, number of components 20 made_num_mixture_components int For MADEs only, number of mixture components 10 made_num_blocks int For MADEs only, number of blocks 4 flow_num_transforms int For flows only, number of transforms 5 Returns: Type Description nn.Module Neural network Source code in sbi/utils/get_nn_models.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 def likelihood_nn ( model : str , theta_shape : torch . Size , x_o_shape : torch . Size , embedding : Optional [ nn . Module ] = None , hidden_features : int = 50 , mdn_num_components : int = 20 , made_num_mixture_components : int = 10 , made_num_blocks : int = 4 , flow_num_transforms : int = 5 , ) -> nn . Module : \"\"\"Neural likelihood density estimator Args: model: Model, one of maf / mdn / made / nsf theta_numel: event shape of the prior, number of parameters. x_o_numel: number of elements in a single data point. embedding: Embedding network hidden_features: For all, number of hidden features mdn_num_components: For MDNs only, number of components made_num_mixture_components: For MADEs only, number of mixture components made_num_blocks: For MADEs only, number of blocks flow_num_transforms: For flows only, number of transforms Returns: Neural network \"\"\" theta_numel = theta_shape . numel () x_o_numel = x_o_shape . numel () if x_o_numel == 1 : _check_1d_flow_limitations ( model , \"data\" ) if model == \"mdn\" : neural_net = MultivariateGaussianMDN ( features = x_o_numel , context_features = theta_numel , hidden_features = hidden_features , hidden_net = nn . Sequential ( nn . Linear ( theta_numel , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Dropout ( p = 0.0 ), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), ), num_components = mdn_num_components , custom_initialization = True , ) elif model == \"made\" : neural_net = MixtureOfGaussiansMADE ( features = x_o_numel , hidden_features = hidden_features , context_features = theta_numel , num_blocks = made_num_blocks , num_mixture_components = made_num_mixture_components , use_residual_blocks = True , random_mask = False , activation = relu , use_batch_norm = True , dropout_probability = 0.0 , custom_initialization = True , ) elif model == \"maf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . MaskedAffineAutoregressiveTransform ( features = x_o_numel , hidden_features = hidden_features , context_features = theta_numel , num_blocks = 2 , use_residual_blocks = False , random_mask = False , activation = tanh , dropout_probability = 0.0 , use_batch_norm = True , ), transforms . RandomPermutation ( features = x_o_numel ), ] ) for _ in range ( flow_num_transforms ) ] ) distribution = distributions_ . StandardNormal (( x_o_numel ,)) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"nsf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . PiecewiseRationalQuadraticCouplingTransform ( mask = create_alternating_binary_mask ( features = x_o_numel , even = ( i % 2 == 0 ) ), transform_net_create_fn = lambda in_features , out_features : nets . ResidualNet ( in_features = in_features , out_features = out_features , hidden_features = hidden_features , context_features = theta_numel , num_blocks = 2 , activation = relu , dropout_probability = 0.0 , use_batch_norm = False , ), num_bins = 10 , tails = \"linear\" , tail_bound = 3.0 , apply_unconditional_transform = False , ), transforms . LULinear ( x_o_numel , identity_init = True ), ] ) for i in range ( flow_num_transforms ) ] ) distribution = distributions_ . StandardNormal (( x_o_numel ,)) neural_net = flows . Flow ( transform , distribution ) else : raise ValueError return neural_net sbi.utils.get_nn_models.classifier_nn ( model , theta_shape , x_o_shape , hidden_features = 50 ) \u00b6 Neural classifier Parameters: Name Type Description Default model Model, one of linear / mlp / resnet required theta_numel event shape of the prior, number of parameters. required x_o_numel number of elements in a single data point. required hidden_features int For all, number of hidden features 50 Returns: Type Description nn.Module Neural network Source code in sbi/utils/get_nn_models.py 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def classifier_nn ( model , theta_shape : torch . Size , x_o_shape : torch . Size , hidden_features : int = 50 , ) -> nn . Module : \"\"\"Neural classifier Args: model: Model, one of linear / mlp / resnet theta_numel: event shape of the prior, number of parameters. x_o_numel: number of elements in a single data point. hidden_features: For all, number of hidden features Returns: Neural network \"\"\" theta_numel = theta_shape . numel () x_o_numel = x_o_shape . numel () if model == \"linear\" : neural_net = nn . Linear ( theta_numel + x_o_numel , 1 ) elif model == \"mlp\" : neural_net = nn . Sequential ( nn . Linear ( theta_numel + x_o_numel , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , 1 ), ) elif model == \"resnet\" : neural_net = nets . ResidualNet ( in_features = theta_numel + x_o_numel , out_features = 1 , hidden_features = hidden_features , context_features = None , num_blocks = 2 , activation = relu , dropout_probability = 0.0 , use_batch_norm = False , ) else : raise ValueError ( f \"'model' must be one of ['linear', 'mlp', 'resnet'].\" ) return neural_net","title":"API Reference"},{"location":"reference/#api-reference","text":"","title":"API Reference"},{"location":"reference/#inference","text":"","title":"Inference"},{"location":"reference/#sbi.inference.snpe.snpe_a.SNPE_A","text":"","title":"SNPE_A"},{"location":"reference/#sbi.inference.snpe.snpe_b.SNPE_B","text":"","title":"SNPE_B"},{"location":"reference/#sbi.inference.snpe.snpe_c.SNPE_C","text":"","title":"SNPE_C"},{"location":"reference/#sbi.inference.snle.snle_a.SNLE_A","text":"SNL=SNLE_A is the only likelihood-based estimator as of now.","title":"SNLE_A"},{"location":"reference/#sbi.inference.snre.snre_a.SNRE_A","text":"AALR[1], here known as SNRE_A. [1] _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_, Hermans et al., ICML 2020, https://arxiv.org/abs/1903.04057","title":"SNRE_A"},{"location":"reference/#sbi.inference.snre.snre_b.SNRE_B","text":"","title":"SNRE_B"},{"location":"reference/#models","text":"","title":"Models"},{"location":"reference/#sbi.utils.get_nn_models.posterior_nn","text":"Neural posterior density estimator Parameters: Name Type Description Default model str Model, one of maf / mdn / made / nsf required prior Prior distribution. required x_o_shape torch.Size Shape of a single observation. Used as input size to the NN. required embedding nn.Module Embedding network Identity() hidden_features int For all, number of hidden features 50 mdn_num_components int For MDNs only, number of components 20 made_num_mixture_components int For MADEs only, number of mixture components 10 made_num_blocks int For MADEs only, number of blocks 4 flow_num_transforms int For flows only, number of transforms 5 Returns: Type Description nn.Module Neural network Source code in sbi/utils/get_nn_models.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def posterior_nn ( model : str , prior , x_o_shape : torch . Size , embedding : nn . Module = nn . Identity (), hidden_features : int = 50 , mdn_num_components : int = 20 , made_num_mixture_components : int = 10 , made_num_blocks : int = 4 , flow_num_transforms : int = 5 , ) -> nn . Module : \"\"\"Neural posterior density estimator Args: model: Model, one of maf / mdn / made / nsf prior: Prior distribution. x_o_shape: Shape of a single observation. Used as input size to the NN. embedding: Embedding network hidden_features: For all, number of hidden features mdn_num_components: For MDNs only, number of components made_num_mixture_components: For MADEs only, number of mixture components made_num_blocks: For MADEs only, number of blocks flow_num_transforms: For flows only, number of transforms Returns: Neural network \"\"\" # We need these asserts because mean and std can be defined outside, prior to user # input checks. prior_mean = prior . mean prior_std = prior . stddev assert ( prior_mean . dtype == float32 ), f \"Prior mean must have dtype float32, is { prior_mean . dtype } .\" assert ( prior_std . dtype == float32 ), f \"Prior std must have dtype float32, is { prior_std . dtype } .\" standardizing_transform = transforms . AffineTransform ( shift =- prior_mean / prior_std , scale = 1 / prior_std ) theta_numel = prior_mean . numel () x_o_numel = x_o_shape . numel () if theta_numel == 1 : _check_1d_flow_limitations ( model , \"parameter\" ) if model == \"mdn\" : neural_net = MultivariateGaussianMDN ( features = theta_numel , context_features = x_o_numel , hidden_features = hidden_features , hidden_net = nn . Sequential ( nn . Linear ( x_o_numel , hidden_features ), nn . ReLU (), nn . Dropout ( p = 0.0 ), nn . Linear ( hidden_features , hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . ReLU (), ), num_components = mdn_num_components , custom_initialization = True , ) elif model == \"made\" : transform = standardizing_transform distribution = distributions_ . MADEMoG ( features = theta_numel , hidden_features = hidden_features , context_features = x_o_numel , num_blocks = made_num_blocks , num_mixture_components = made_num_mixture_components , use_residual_blocks = True , random_mask = False , activation = relu , dropout_probability = 0.0 , use_batch_norm = False , custom_initialization = True , ) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"maf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . MaskedAffineAutoregressiveTransform ( features = theta_numel , hidden_features = hidden_features , context_features = x_o_numel , num_blocks = 2 , use_residual_blocks = False , random_mask = False , activation = tanh , dropout_probability = 0.0 , use_batch_norm = True , ), transforms . RandomPermutation ( features = theta_numel ), ] ) for _ in range ( flow_num_transforms ) ] ) transform = transforms . CompositeTransform ([ standardizing_transform , transform ,]) distribution = distributions_ . StandardNormal (( theta_numel ,)) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"nsf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . PiecewiseRationalQuadraticCouplingTransform ( mask = create_alternating_binary_mask ( features = theta_numel , even = ( i % 2 == 0 ) ), transform_net_create_fn = lambda in_features , out_features : nets . ResidualNet ( in_features = in_features , out_features = out_features , hidden_features = hidden_features , context_features = x_o_numel , num_blocks = 2 , activation = relu , dropout_probability = 0.0 , use_batch_norm = False , ), num_bins = 10 , tails = \"linear\" , tail_bound = 3.0 , apply_unconditional_transform = False , ), transforms . LULinear ( theta_numel , identity_init = True ), ] ) for i in range ( flow_num_transforms ) ] ) transform = transforms . CompositeTransform ([ standardizing_transform , transform ,]) distribution = distributions_ . StandardNormal (( theta_numel ,)) neural_net = flows . Flow ( transform , distribution , embedding ) else : raise ValueError return neural_net","title":"posterior_nn()"},{"location":"reference/#sbi.utils.get_nn_models.likelihood_nn","text":"Neural likelihood density estimator Parameters: Name Type Description Default model str Model, one of maf / mdn / made / nsf required theta_numel event shape of the prior, number of parameters. required x_o_numel number of elements in a single data point. required embedding Optional[nn.Module] Embedding network None hidden_features int For all, number of hidden features 50 mdn_num_components int For MDNs only, number of components 20 made_num_mixture_components int For MADEs only, number of mixture components 10 made_num_blocks int For MADEs only, number of blocks 4 flow_num_transforms int For flows only, number of transforms 5 Returns: Type Description nn.Module Neural network Source code in sbi/utils/get_nn_models.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 def likelihood_nn ( model : str , theta_shape : torch . Size , x_o_shape : torch . Size , embedding : Optional [ nn . Module ] = None , hidden_features : int = 50 , mdn_num_components : int = 20 , made_num_mixture_components : int = 10 , made_num_blocks : int = 4 , flow_num_transforms : int = 5 , ) -> nn . Module : \"\"\"Neural likelihood density estimator Args: model: Model, one of maf / mdn / made / nsf theta_numel: event shape of the prior, number of parameters. x_o_numel: number of elements in a single data point. embedding: Embedding network hidden_features: For all, number of hidden features mdn_num_components: For MDNs only, number of components made_num_mixture_components: For MADEs only, number of mixture components made_num_blocks: For MADEs only, number of blocks flow_num_transforms: For flows only, number of transforms Returns: Neural network \"\"\" theta_numel = theta_shape . numel () x_o_numel = x_o_shape . numel () if x_o_numel == 1 : _check_1d_flow_limitations ( model , \"data\" ) if model == \"mdn\" : neural_net = MultivariateGaussianMDN ( features = x_o_numel , context_features = theta_numel , hidden_features = hidden_features , hidden_net = nn . Sequential ( nn . Linear ( theta_numel , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Dropout ( p = 0.0 ), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), ), num_components = mdn_num_components , custom_initialization = True , ) elif model == \"made\" : neural_net = MixtureOfGaussiansMADE ( features = x_o_numel , hidden_features = hidden_features , context_features = theta_numel , num_blocks = made_num_blocks , num_mixture_components = made_num_mixture_components , use_residual_blocks = True , random_mask = False , activation = relu , use_batch_norm = True , dropout_probability = 0.0 , custom_initialization = True , ) elif model == \"maf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . MaskedAffineAutoregressiveTransform ( features = x_o_numel , hidden_features = hidden_features , context_features = theta_numel , num_blocks = 2 , use_residual_blocks = False , random_mask = False , activation = tanh , dropout_probability = 0.0 , use_batch_norm = True , ), transforms . RandomPermutation ( features = x_o_numel ), ] ) for _ in range ( flow_num_transforms ) ] ) distribution = distributions_ . StandardNormal (( x_o_numel ,)) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"nsf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . PiecewiseRationalQuadraticCouplingTransform ( mask = create_alternating_binary_mask ( features = x_o_numel , even = ( i % 2 == 0 ) ), transform_net_create_fn = lambda in_features , out_features : nets . ResidualNet ( in_features = in_features , out_features = out_features , hidden_features = hidden_features , context_features = theta_numel , num_blocks = 2 , activation = relu , dropout_probability = 0.0 , use_batch_norm = False , ), num_bins = 10 , tails = \"linear\" , tail_bound = 3.0 , apply_unconditional_transform = False , ), transforms . LULinear ( x_o_numel , identity_init = True ), ] ) for i in range ( flow_num_transforms ) ] ) distribution = distributions_ . StandardNormal (( x_o_numel ,)) neural_net = flows . Flow ( transform , distribution ) else : raise ValueError return neural_net","title":"likelihood_nn()"},{"location":"reference/#sbi.utils.get_nn_models.classifier_nn","text":"Neural classifier Parameters: Name Type Description Default model Model, one of linear / mlp / resnet required theta_numel event shape of the prior, number of parameters. required x_o_numel number of elements in a single data point. required hidden_features int For all, number of hidden features 50 Returns: Type Description nn.Module Neural network Source code in sbi/utils/get_nn_models.py 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def classifier_nn ( model , theta_shape : torch . Size , x_o_shape : torch . Size , hidden_features : int = 50 , ) -> nn . Module : \"\"\"Neural classifier Args: model: Model, one of linear / mlp / resnet theta_numel: event shape of the prior, number of parameters. x_o_numel: number of elements in a single data point. hidden_features: For all, number of hidden features Returns: Neural network \"\"\" theta_numel = theta_shape . numel () x_o_numel = x_o_shape . numel () if model == \"linear\" : neural_net = nn . Linear ( theta_numel + x_o_numel , 1 ) elif model == \"mlp\" : neural_net = nn . Sequential ( nn . Linear ( theta_numel + x_o_numel , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , 1 ), ) elif model == \"resnet\" : neural_net = nets . ResidualNet ( in_features = theta_numel + x_o_numel , out_features = 1 , hidden_features = hidden_features , context_features = None , num_blocks = 2 , activation = relu , dropout_probability = 0.0 , use_batch_norm = False , ) else : raise ValueError ( f \"'model' must be one of ['linear', 'mlp', 'resnet'].\" ) return neural_net","title":"classifier_nn()"},{"location":"tutorial/markdown_files/00_getting_started/","text":"Getting started with sbi \u00b6 import torch import sbi.utils as utils from sbi.inference.base import infer Running the inference procedure \u00b6 sbi provides a simple interface to run state-of-the-art algorithms for simulation-based inference. For inference, you need to provide two things: 1) a prior distribution that allows to sample parameter sets. 2) a simulator that takes parameter sets and produces simulation outputs. For example, we can have a 3-dimensional parameter space with a uniform prior between [-1,1] and a simulator that adds 1.0 and some Gaussian noise to the parameter set: num_dim = 3 prior = utils . BoxUniform ( - 2 * torch . ones ( num_dim ), 2 * torch . ones ( num_dim )) def simulator ( parameter_set ): return 1.0 + parameter_set + torch . randn ( parameter_set . shape ) * 0.1 sbi can then run inference: posterior = infer ( simulator , prior , 'SNPE' , num_simulations = 1000 ) Let\u2019s say we have made some observation \\(x\\) : observation = torch . zeros ( 3 ) Given this observation, we can then sample from the posterior \\(p(\\theta|x)\\) , evaluate its log-probability, or plot it. samples = posterior . sample (( 10000 ,), x = observation ) log_probability = posterior . log_prob ( samples , x = observation ) _ = utils . pairplot ( samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 6 , 6 )) Requirements for the simulator, prior, and observation \u00b6 For all algorithms, all you need to provide are a prior and a simulator. Let\u2019s talk about what requirements they need to satisfy. Prior \u00b6 A distribution that allows to sample parameter sets. Any datatype for the prior is allowed as long as it allows to call prior.sample() and prior.log_prob() . Simulator \u00b6 A python callable that takes in a parameter set and outputs data with some (even if very small) stochasticity. Allowed data types and shapes for input and output: - the input parameter set and the output have to be either a np.ndarray or a torch.Tensor . - the input parameter set should have either shape (1,N) or (N) , and the output must have shape (1,M) or (M) . Observation \u00b6 An observation \\(x_o\\) for which you want to infer the posterior \\(p(\\theta|x_o)\\) . Allowed data types and shapes: - either a np.ndarray or a torch.Tensor . - shape must be either (1,N) or (N) . Running different algorithms \u00b6 sbi implements three classes of algorithms that can be used to obtain the posterior distribution: SNPE, SNL, and SRE. You can try the different algorithms by simply swapping out the method : posterior = infer ( simulator , prior , 'SNPE' , num_simulations = 1000 ) posterior = infer ( simulator , prior , 'SNLE' , num_simulations = 1000 ) posterior = infer ( simulator , prior , 'SNRE' , num_simulations = 1000 ) You can then infer, sample, evaluate, and plot the posterior as described above.","title":"Getting started"},{"location":"tutorial/markdown_files/00_getting_started/#getting-started-with-sbi","text":"import torch import sbi.utils as utils from sbi.inference.base import infer","title":"Getting started with sbi"},{"location":"tutorial/markdown_files/00_getting_started/#running-the-inference-procedure","text":"sbi provides a simple interface to run state-of-the-art algorithms for simulation-based inference. For inference, you need to provide two things: 1) a prior distribution that allows to sample parameter sets. 2) a simulator that takes parameter sets and produces simulation outputs. For example, we can have a 3-dimensional parameter space with a uniform prior between [-1,1] and a simulator that adds 1.0 and some Gaussian noise to the parameter set: num_dim = 3 prior = utils . BoxUniform ( - 2 * torch . ones ( num_dim ), 2 * torch . ones ( num_dim )) def simulator ( parameter_set ): return 1.0 + parameter_set + torch . randn ( parameter_set . shape ) * 0.1 sbi can then run inference: posterior = infer ( simulator , prior , 'SNPE' , num_simulations = 1000 ) Let\u2019s say we have made some observation \\(x\\) : observation = torch . zeros ( 3 ) Given this observation, we can then sample from the posterior \\(p(\\theta|x)\\) , evaluate its log-probability, or plot it. samples = posterior . sample (( 10000 ,), x = observation ) log_probability = posterior . log_prob ( samples , x = observation ) _ = utils . pairplot ( samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 6 , 6 ))","title":"Running the inference procedure"},{"location":"tutorial/markdown_files/00_getting_started/#requirements-for-the-simulator-prior-and-observation","text":"For all algorithms, all you need to provide are a prior and a simulator. Let\u2019s talk about what requirements they need to satisfy.","title":"Requirements for the simulator, prior, and observation"},{"location":"tutorial/markdown_files/00_getting_started/#prior","text":"A distribution that allows to sample parameter sets. Any datatype for the prior is allowed as long as it allows to call prior.sample() and prior.log_prob() .","title":"Prior"},{"location":"tutorial/markdown_files/00_getting_started/#simulator","text":"A python callable that takes in a parameter set and outputs data with some (even if very small) stochasticity. Allowed data types and shapes for input and output: - the input parameter set and the output have to be either a np.ndarray or a torch.Tensor . - the input parameter set should have either shape (1,N) or (N) , and the output must have shape (1,M) or (M) .","title":"Simulator"},{"location":"tutorial/markdown_files/00_getting_started/#observation","text":"An observation \\(x_o\\) for which you want to infer the posterior \\(p(\\theta|x_o)\\) . Allowed data types and shapes: - either a np.ndarray or a torch.Tensor . - shape must be either (1,N) or (N) .","title":"Observation"},{"location":"tutorial/markdown_files/00_getting_started/#running-different-algorithms","text":"sbi implements three classes of algorithms that can be used to obtain the posterior distribution: SNPE, SNL, and SRE. You can try the different algorithms by simply swapping out the method : posterior = infer ( simulator , prior , 'SNPE' , num_simulations = 1000 ) posterior = infer ( simulator , prior , 'SNLE' , num_simulations = 1000 ) posterior = infer ( simulator , prior , 'SNRE' , num_simulations = 1000 ) You can then infer, sample, evaluate, and plot the posterior as described above.","title":"Running different algorithms"},{"location":"tutorial/markdown_files/01_gaussian_amortized/","text":"Amortized posterior inference on Gaussian example \u00b6 In this tutorial, we will demonstrate how sbi can infer an amortized posterior for a simple toy model with a uniform prior and Gaussian likelihood. import torch import numpy as np import sbi.utils as utils from sbi.inference.base import infer Defining prior, simulator, and running inference \u00b6 Say we have 3-dimensional parameter space, and the prior is uniformly distributed between -2 and 2 in each dimension. num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) Our simulator takes the input parameters, adds 1.0 in each dimension, and then adds some Gaussian noise: def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 We can then run inference: posterior = infer ( linear_gaussian , prior , 'SNPE' , num_simulations = 1000 ) Amortized inference \u00b6 As it can be seen above, we have not yet provided an observation to the inference procedure. In fact, we can evaluate the posterior for different observations without having to re-run inference. This is called amortization. Let\u2019s say we have two observations x_o_1 = [0,0,0] and x_o_2 = [2,2,2] : x_o_1 = torch . zeros ( 3 ,) x_o_2 = 2.0 * torch . ones ( 3 ,) We can draw samples from the posterior given x_o_1 and then plot them: posterior_samples_1 = posterior . sample (( 10000 ,), x = x_o_1 ) # plot posterior samples _ = utils . pairplot ( posterior_samples_1 , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 )) As it can be seen, the posterior samples are centered around [-1,-1,-1] in each dimension. This makes sense because the simulator always adds 1.0 in each dimension and we have observed x_o_1 = [0,0,0] . Since the obtained posterior is amortized, we can also draw samples from the posterior given the second observation without having to re-run interence: posterior_samples_2 = posterior . sample (( 10000 ,), x = x_o_2 ) # plot posterior samples _ = utils . pairplot ( posterior_samples_2 , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 )) So, if we have observed x_o_2 = [2,2,2] , the posterior is centered around [1,1,1] \u2013 again, this makes sense because the simulator adds 1.0 in each dimension.","title":"Amortized inference"},{"location":"tutorial/markdown_files/01_gaussian_amortized/#amortized-posterior-inference-on-gaussian-example","text":"In this tutorial, we will demonstrate how sbi can infer an amortized posterior for a simple toy model with a uniform prior and Gaussian likelihood. import torch import numpy as np import sbi.utils as utils from sbi.inference.base import infer","title":"Amortized posterior inference on Gaussian example"},{"location":"tutorial/markdown_files/01_gaussian_amortized/#defining-prior-simulator-and-running-inference","text":"Say we have 3-dimensional parameter space, and the prior is uniformly distributed between -2 and 2 in each dimension. num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) Our simulator takes the input parameters, adds 1.0 in each dimension, and then adds some Gaussian noise: def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 We can then run inference: posterior = infer ( linear_gaussian , prior , 'SNPE' , num_simulations = 1000 )","title":"Defining prior, simulator, and running inference"},{"location":"tutorial/markdown_files/01_gaussian_amortized/#amortized-inference","text":"As it can be seen above, we have not yet provided an observation to the inference procedure. In fact, we can evaluate the posterior for different observations without having to re-run inference. This is called amortization. Let\u2019s say we have two observations x_o_1 = [0,0,0] and x_o_2 = [2,2,2] : x_o_1 = torch . zeros ( 3 ,) x_o_2 = 2.0 * torch . ones ( 3 ,) We can draw samples from the posterior given x_o_1 and then plot them: posterior_samples_1 = posterior . sample (( 10000 ,), x = x_o_1 ) # plot posterior samples _ = utils . pairplot ( posterior_samples_1 , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 )) As it can be seen, the posterior samples are centered around [-1,-1,-1] in each dimension. This makes sense because the simulator always adds 1.0 in each dimension and we have observed x_o_1 = [0,0,0] . Since the obtained posterior is amortized, we can also draw samples from the posterior given the second observation without having to re-run interence: posterior_samples_2 = posterior . sample (( 10000 ,), x = x_o_2 ) # plot posterior samples _ = utils . pairplot ( posterior_samples_2 , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 )) So, if we have observed x_o_2 = [2,2,2] , the posterior is centered around [1,1,1] \u2013 again, this makes sense because the simulator adds 1.0 in each dimension.","title":"Amortized inference"},{"location":"tutorial/markdown_files/02_HH_simulator/","text":"Inference on Hodgkin-Huxley model: tutorial \u00b6 In this tutorial, we use sbi to do inference on a Hodgkin-Huxley model (Hodgkin and Huxley, 1952) with two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given a current-clamp recording (synthetically generated). First we are going to import basic packages. import numpy as np import torch # VISUALIZATION import matplotlib as mpl import matplotlib.pyplot as plt # sbi import sbi import sbi.utils as utils from sbi.inference.base import infer # remove top and right axis from plots mpl . rcParams [ 'axes.spines.right' ] = False mpl . rcParams [ 'axes.spines.top' ] = False Different required components \u00b6 Before running inference, let us define the different required components: observed data prior over model parameters simulator 1) Observed data \u00b6 Let us assume we current-clamped a neuron and recorded the following voltage trace: In fact, this voltage trace was not measured experimentally but synthetically generated by simulating a Hodgkin-Huxley model with particular parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). We will come back to this point later in the tutorial. 2) Simulator \u00b6 We would like to infer the posterior over the two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ) of a Hodgkin-Huxley model, given the observed electrophysiological recording above. The model has channel kinetics as in Pospischil et al. 2008, and is defined by the following set of differential equations (parameters of interest highlighted in orange): \\[ \\scriptsize \\begin{align} C_m\\frac{dV}{dt}&=g_1\\left(E_1-V\\right)+ \\color{orange}{\\bar{g}_{Na}}m^3h\\left(E_{Na}-V\\right)+ \\color{orange}{\\bar{g}_{K}}n^4\\left(E_K-V\\right)+ \\bar{g}_Mp\\left(E_K-V\\right)+ I_{inj}+ \\sigma\\eta\\left(t\\right)\\\\ \\frac{dq}{dt}&=\\frac{q_\\infty\\left(V\\right)-q}{\\tau_q\\left(V\\right)},\\;q\\in\\{m,h,n,p\\} \\end{align} \\] where \\(V\\) is the membrane potential, \\(C_m\\) is the membrane capacitance, \\(g_{\\text{l}}\\) is the leak conductance, \\(E_{\\text{l}}\\) is the membrane reversal potential, \\(\\bar{g}_c\\) is the density of channels of type \\(c\\) ( \\(\\text{Na}^+\\) , \\(\\text{K}^+\\) , M), \\(E_c\\) is the reversal potential of \\(c\\) , ( \\(m\\) , \\(h\\) , \\(n\\) , \\(p\\) ) are the respective channel gating kinetic variables, and \\(\\sigma \\eta(t)\\) is the intrinsic neural noise. The right hand side of the voltage dynamics is composed of a leak current, a voltage-dependent \\(\\text{Na}^+\\) current, a delayed-rectifier \\(\\text{K}^+\\) current, a slow voltage-dependent \\(\\text{K}^+\\) current responsible for spike-frequency adaptation, and an injected current \\(I_{\\text{inj}}\\) . Channel gating variables \\(q\\) have dynamics fully characterized by the neuron membrane potential \\(V\\) , given the respective steady-state \\(q_{\\infty}(V)\\) and time constant \\(\\tau_{q}(V)\\) (details in Pospischil et al. 2008). The input current \\(I_{\\text{inj}}\\) is defined as from HH_helper_functions import syn_current I , t_on , t_off , dt , t , A_soma = syn_current () The Hodgkin-Huxley simulator is given by: from HH_helper_functions import HHsimulator Putting the input current and the simulator together: def run_HH_model ( params ): # input current, time step I , t_on , t_off , dt , t , A_soma = syn_current () t = np . arange ( 0 , len ( I ), 1 ) * dt # initial voltage V0 = - 70 params = np . asarray ( params ) states = HHsimulator ( V0 , params . reshape ( 1 , - 1 ), dt , t , I , seed = 0 ) return { 'data' : states . reshape ( - 1 ), 'time' : t , 'dt' : dt , 'I' : I . reshape ( - 1 )} To get an idea of the output of the Hodgkin-Huxley model, let us generate some voltage traces for different parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the input current \\(I_{\\text{inj}}\\) : # three sets of (g_Na, g_K) params = np . array ([[ 50. , 1. ],[ 4. , 1.5 ],[ 20. , 15. ]]) num_samples = len ( params [:, 0 ]) sim_samples = np . zeros (( num_samples , len ( I ))) for i in range ( num_samples ): sim_samples [ i ,:] = run_HH_model ( params = params [ i ,:])[ 'data' ] # colors for traces col_min = 2 num_colors = num_samples + col_min cm1 = mpl . cm . Blues col1 = [ cm1 ( 1. * i / num_colors ) for i in range ( col_min , num_colors )] fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) for i in range ( num_samples ): plt . plot ( t , sim_samples [ i ,:], color = col1 [ i ], lw = 2 ) plt . ylabel ( 'voltage (mV)' ) ax . set_xticks ([]) ax . set_yticks ([ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( t , I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( t ) / 2 , max ( t )]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' )) As can be seen, the voltage traces can be quite diverse for different parameter values. Often, we are not interested in matching the exact trace, but only in matching certain features thereof. In this example of the Hodgkin Huxley model, the summary features are the number of spikes, the mean resting potential, the standard deviation of the resting potential, and the first 4 voltage moments, mean, standard deviation, skewness and kurtosis. In the function calculate_summary_statistics() below, we compute these statistics from the output of the Hodgkin Huxley simulator. from HH_helper_functions import calculate_summary_statistics Lastly, we define a function that performs all of the above steps at once. The function simulation_wrapper takes in conductance values, runs the Hodgkin Huxley model and then returns the summary statistics. def simulation_wrapper ( params ): \"\"\" Takes in conductance values and then first runs the Hodgkin Huxley model and then returns the summary statistics as torch.Tensor \"\"\" obs = run_HH_model ( params ) summstats = torch . as_tensor ( calculate_summary_statistics ( obs )) return summstats sbi takes any function as simulator. Thus, sbi also has the flexibility to use simulators that utilize external packages, e.g., Brian ( http://briansimulator.org/ ), nest ( https://www.nest-simulator.org/ ), or NEURON ( https://neuron.yale.edu/neuron/ ). External simulators do not even need to be Python-based as long as they store simulation outputs in a format that can be read from Python. All that is necessary is to wrap your external simulator of choice into a python callable that takes a parameter set and outputs a set of summary statistics we want to fit the parameters to. 3) Prior over model parameters \u00b6 Now that we have the simulator, we need to define a function with the prior over the model parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), which in this case is chosen to be a Uniform distribution: prior_min = [ . 5 , 1e-4 ] prior_max = [ 80. , 15. ] prior = utils . torchutils . BoxUniform ( low = torch . as_tensor ( prior_min ), high = torch . as_tensor ( prior_max )) Inference \u00b6 Now that we have all the required components, we can run inference with SNPE. We start by importing our SNPE object of choice. We will now perform inference with SNPE to identify parameters whose activity matches this trace: posterior = infer ( simulation_wrapper , prior , 'SNPE' , num_simulations = 300 , num_workers = 4 ) Coming back to the observed data \u00b6 As mentioned at the beginning of the tutorial, the observed data are generated by the Hodgkin-Huxley model with a set of known parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). To illustrate how to compute the summary statistics of the observed data, let us regenerate the observed data: # true parameters and respective labels true_params = np . array ([ 50. , 5. ]) labels_params = [ r '$g_ {Na} $' , r '$g_ {K} $' ] observation_trace = run_HH_model ( true_params ) observation_summary_statistics = calculate_summary_statistics ( observation_trace ) As we had already shown above, the observed voltage traces look as follows: fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) plt . plot ( observation_trace [ 'time' ], observation_trace [ 'data' ]) plt . ylabel ( 'voltage (mV)' ) plt . title ( 'observed data' ) ax . set_xticks ([]) ax . set_yticks ([ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( observation_trace [ 'time' ], I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( observation_trace [ 'time' ]) / 2 , max ( observation_trace [ 'time' ])]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' )) Analysis of the posterior given the observed data \u00b6 After running the inference algorithm, let us inspect the inferred posterior distribution over the parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the observed trace. To do so, we first draw samples (i.e. consistent parameter sets) from the posterior: samples = posterior . sample (( 10000 ,), x = observation_summary_statistics ) fig , axes = utils . pairplot ( samples , limits = [[ . 5 , 80 ], [ 1e-4 , 15. ]], ticks = [[ . 5 , 80 ], [ 1e-4 , 15. ]], fig_size = ( 5 , 5 ), points = true_params , points_offdiag = { 'markersize' : 10 }, points_colors = 'r' ); As can be seen, the inferred posterior contains the ground-truth parameters (red) in a high-probability region. Now, let us sample parameters from the posterior distribution, simulate the Hodgkin-Huxley model for this parameter set and compare the simulations with the observed data: # draw a sample from the posterior and convert to numpy posterior_sample = posterior . sample ( num_samples = 1 , x = observation_summary_statistics ) . numpy () fig = plt . figure ( figsize = ( 7 , 5 )) # plot observation t = observation_trace [ 'time' ] y_obs = observation_trace [ 'data' ] plt . plot ( t , y_obs , lw = 2 , label = 'observation' ) # simulate and plot samples x = run_HH_model ( posterior_sample ) plt . plot ( t , x [ 'data' ], '--' , lw = 2 , label = 'posterior sample' ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'voltage (mV)' ) ax = plt . gca () handles , labels = ax . get_legend_handles_labels () ax . legend ( handles [:: - 1 ], labels [:: - 1 ], bbox_to_anchor = ( 1.3 , 1 ), loc = 'upper right' ) ax . set_xticks ([ 0 , 60 , 120 ]) ax . set_yticks ([ - 80 , - 20 , 40 ]); As can be seen, the sample from the inferred posterior leads to simulations that closely resemble the observed data, confirming that SNPE did a good job at capturing the observed data. References \u00b6 A. L. Hodgkin and A. F. Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. The Journal of Physiology, 117(4):500\u2013544, 1952. M. Pospischil, M. Toledo-Rodriguez, C. Monier, Z. Piwkowska, T. Bal, Y. Fr\u00e9gnac, H. Markram, and A. Destexhe. Minimal Hodgkin-Huxley type models for different classes of cortical and thalamic neurons. Biological Cybernetics, 99(4-5), 2008.","title":"Hodgkin-Huxley example"},{"location":"tutorial/markdown_files/02_HH_simulator/#inference-on-hodgkin-huxley-model-tutorial","text":"In this tutorial, we use sbi to do inference on a Hodgkin-Huxley model (Hodgkin and Huxley, 1952) with two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given a current-clamp recording (synthetically generated). First we are going to import basic packages. import numpy as np import torch # VISUALIZATION import matplotlib as mpl import matplotlib.pyplot as plt # sbi import sbi import sbi.utils as utils from sbi.inference.base import infer # remove top and right axis from plots mpl . rcParams [ 'axes.spines.right' ] = False mpl . rcParams [ 'axes.spines.top' ] = False","title":"Inference on Hodgkin-Huxley model: tutorial"},{"location":"tutorial/markdown_files/02_HH_simulator/#different-required-components","text":"Before running inference, let us define the different required components: observed data prior over model parameters simulator","title":"Different required components"},{"location":"tutorial/markdown_files/02_HH_simulator/#1-observed-data","text":"Let us assume we current-clamped a neuron and recorded the following voltage trace: In fact, this voltage trace was not measured experimentally but synthetically generated by simulating a Hodgkin-Huxley model with particular parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). We will come back to this point later in the tutorial.","title":"1) Observed data"},{"location":"tutorial/markdown_files/02_HH_simulator/#2-simulator","text":"We would like to infer the posterior over the two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ) of a Hodgkin-Huxley model, given the observed electrophysiological recording above. The model has channel kinetics as in Pospischil et al. 2008, and is defined by the following set of differential equations (parameters of interest highlighted in orange): \\[ \\scriptsize \\begin{align} C_m\\frac{dV}{dt}&=g_1\\left(E_1-V\\right)+ \\color{orange}{\\bar{g}_{Na}}m^3h\\left(E_{Na}-V\\right)+ \\color{orange}{\\bar{g}_{K}}n^4\\left(E_K-V\\right)+ \\bar{g}_Mp\\left(E_K-V\\right)+ I_{inj}+ \\sigma\\eta\\left(t\\right)\\\\ \\frac{dq}{dt}&=\\frac{q_\\infty\\left(V\\right)-q}{\\tau_q\\left(V\\right)},\\;q\\in\\{m,h,n,p\\} \\end{align} \\] where \\(V\\) is the membrane potential, \\(C_m\\) is the membrane capacitance, \\(g_{\\text{l}}\\) is the leak conductance, \\(E_{\\text{l}}\\) is the membrane reversal potential, \\(\\bar{g}_c\\) is the density of channels of type \\(c\\) ( \\(\\text{Na}^+\\) , \\(\\text{K}^+\\) , M), \\(E_c\\) is the reversal potential of \\(c\\) , ( \\(m\\) , \\(h\\) , \\(n\\) , \\(p\\) ) are the respective channel gating kinetic variables, and \\(\\sigma \\eta(t)\\) is the intrinsic neural noise. The right hand side of the voltage dynamics is composed of a leak current, a voltage-dependent \\(\\text{Na}^+\\) current, a delayed-rectifier \\(\\text{K}^+\\) current, a slow voltage-dependent \\(\\text{K}^+\\) current responsible for spike-frequency adaptation, and an injected current \\(I_{\\text{inj}}\\) . Channel gating variables \\(q\\) have dynamics fully characterized by the neuron membrane potential \\(V\\) , given the respective steady-state \\(q_{\\infty}(V)\\) and time constant \\(\\tau_{q}(V)\\) (details in Pospischil et al. 2008). The input current \\(I_{\\text{inj}}\\) is defined as from HH_helper_functions import syn_current I , t_on , t_off , dt , t , A_soma = syn_current () The Hodgkin-Huxley simulator is given by: from HH_helper_functions import HHsimulator Putting the input current and the simulator together: def run_HH_model ( params ): # input current, time step I , t_on , t_off , dt , t , A_soma = syn_current () t = np . arange ( 0 , len ( I ), 1 ) * dt # initial voltage V0 = - 70 params = np . asarray ( params ) states = HHsimulator ( V0 , params . reshape ( 1 , - 1 ), dt , t , I , seed = 0 ) return { 'data' : states . reshape ( - 1 ), 'time' : t , 'dt' : dt , 'I' : I . reshape ( - 1 )} To get an idea of the output of the Hodgkin-Huxley model, let us generate some voltage traces for different parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the input current \\(I_{\\text{inj}}\\) : # three sets of (g_Na, g_K) params = np . array ([[ 50. , 1. ],[ 4. , 1.5 ],[ 20. , 15. ]]) num_samples = len ( params [:, 0 ]) sim_samples = np . zeros (( num_samples , len ( I ))) for i in range ( num_samples ): sim_samples [ i ,:] = run_HH_model ( params = params [ i ,:])[ 'data' ] # colors for traces col_min = 2 num_colors = num_samples + col_min cm1 = mpl . cm . Blues col1 = [ cm1 ( 1. * i / num_colors ) for i in range ( col_min , num_colors )] fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) for i in range ( num_samples ): plt . plot ( t , sim_samples [ i ,:], color = col1 [ i ], lw = 2 ) plt . ylabel ( 'voltage (mV)' ) ax . set_xticks ([]) ax . set_yticks ([ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( t , I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( t ) / 2 , max ( t )]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' )) As can be seen, the voltage traces can be quite diverse for different parameter values. Often, we are not interested in matching the exact trace, but only in matching certain features thereof. In this example of the Hodgkin Huxley model, the summary features are the number of spikes, the mean resting potential, the standard deviation of the resting potential, and the first 4 voltage moments, mean, standard deviation, skewness and kurtosis. In the function calculate_summary_statistics() below, we compute these statistics from the output of the Hodgkin Huxley simulator. from HH_helper_functions import calculate_summary_statistics Lastly, we define a function that performs all of the above steps at once. The function simulation_wrapper takes in conductance values, runs the Hodgkin Huxley model and then returns the summary statistics. def simulation_wrapper ( params ): \"\"\" Takes in conductance values and then first runs the Hodgkin Huxley model and then returns the summary statistics as torch.Tensor \"\"\" obs = run_HH_model ( params ) summstats = torch . as_tensor ( calculate_summary_statistics ( obs )) return summstats sbi takes any function as simulator. Thus, sbi also has the flexibility to use simulators that utilize external packages, e.g., Brian ( http://briansimulator.org/ ), nest ( https://www.nest-simulator.org/ ), or NEURON ( https://neuron.yale.edu/neuron/ ). External simulators do not even need to be Python-based as long as they store simulation outputs in a format that can be read from Python. All that is necessary is to wrap your external simulator of choice into a python callable that takes a parameter set and outputs a set of summary statistics we want to fit the parameters to.","title":"2) Simulator"},{"location":"tutorial/markdown_files/02_HH_simulator/#3-prior-over-model-parameters","text":"Now that we have the simulator, we need to define a function with the prior over the model parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), which in this case is chosen to be a Uniform distribution: prior_min = [ . 5 , 1e-4 ] prior_max = [ 80. , 15. ] prior = utils . torchutils . BoxUniform ( low = torch . as_tensor ( prior_min ), high = torch . as_tensor ( prior_max ))","title":"3) Prior over model parameters"},{"location":"tutorial/markdown_files/02_HH_simulator/#inference","text":"Now that we have all the required components, we can run inference with SNPE. We start by importing our SNPE object of choice. We will now perform inference with SNPE to identify parameters whose activity matches this trace: posterior = infer ( simulation_wrapper , prior , 'SNPE' , num_simulations = 300 , num_workers = 4 )","title":"Inference"},{"location":"tutorial/markdown_files/02_HH_simulator/#coming-back-to-the-observed-data","text":"As mentioned at the beginning of the tutorial, the observed data are generated by the Hodgkin-Huxley model with a set of known parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). To illustrate how to compute the summary statistics of the observed data, let us regenerate the observed data: # true parameters and respective labels true_params = np . array ([ 50. , 5. ]) labels_params = [ r '$g_ {Na} $' , r '$g_ {K} $' ] observation_trace = run_HH_model ( true_params ) observation_summary_statistics = calculate_summary_statistics ( observation_trace ) As we had already shown above, the observed voltage traces look as follows: fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) plt . plot ( observation_trace [ 'time' ], observation_trace [ 'data' ]) plt . ylabel ( 'voltage (mV)' ) plt . title ( 'observed data' ) ax . set_xticks ([]) ax . set_yticks ([ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( observation_trace [ 'time' ], I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( observation_trace [ 'time' ]) / 2 , max ( observation_trace [ 'time' ])]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' ))","title":"Coming back to the observed data"},{"location":"tutorial/markdown_files/02_HH_simulator/#analysis-of-the-posterior-given-the-observed-data","text":"After running the inference algorithm, let us inspect the inferred posterior distribution over the parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the observed trace. To do so, we first draw samples (i.e. consistent parameter sets) from the posterior: samples = posterior . sample (( 10000 ,), x = observation_summary_statistics ) fig , axes = utils . pairplot ( samples , limits = [[ . 5 , 80 ], [ 1e-4 , 15. ]], ticks = [[ . 5 , 80 ], [ 1e-4 , 15. ]], fig_size = ( 5 , 5 ), points = true_params , points_offdiag = { 'markersize' : 10 }, points_colors = 'r' ); As can be seen, the inferred posterior contains the ground-truth parameters (red) in a high-probability region. Now, let us sample parameters from the posterior distribution, simulate the Hodgkin-Huxley model for this parameter set and compare the simulations with the observed data: # draw a sample from the posterior and convert to numpy posterior_sample = posterior . sample ( num_samples = 1 , x = observation_summary_statistics ) . numpy () fig = plt . figure ( figsize = ( 7 , 5 )) # plot observation t = observation_trace [ 'time' ] y_obs = observation_trace [ 'data' ] plt . plot ( t , y_obs , lw = 2 , label = 'observation' ) # simulate and plot samples x = run_HH_model ( posterior_sample ) plt . plot ( t , x [ 'data' ], '--' , lw = 2 , label = 'posterior sample' ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'voltage (mV)' ) ax = plt . gca () handles , labels = ax . get_legend_handles_labels () ax . legend ( handles [:: - 1 ], labels [:: - 1 ], bbox_to_anchor = ( 1.3 , 1 ), loc = 'upper right' ) ax . set_xticks ([ 0 , 60 , 120 ]) ax . set_yticks ([ - 80 , - 20 , 40 ]); As can be seen, the sample from the inferred posterior leads to simulations that closely resemble the observed data, confirming that SNPE did a good job at capturing the observed data.","title":"Analysis of the posterior given the observed data"},{"location":"tutorial/markdown_files/02_HH_simulator/#references","text":"A. L. Hodgkin and A. F. Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. The Journal of Physiology, 117(4):500\u2013544, 1952. M. Pospischil, M. Toledo-Rodriguez, C. Monier, Z. Piwkowska, T. Bal, Y. Fr\u00e9gnac, H. Markram, and A. Destexhe. Minimal Hodgkin-Huxley type models for different classes of cortical and thalamic neurons. Biological Cybernetics, 99(4-5), 2008.","title":"References"},{"location":"tutorial/markdown_files/03_flexible_interface/","text":"The flexible interface \u00b6 In the previous tutorial, we have demonstrated how sbi can be used to run simulation-based inference with just a single line of code. In addition to this simple interface, sbi also provides a flexible interface which unlocks several additional features implemented in sbi . Features \u00b6 The flexible interface allows you to customize the following: performing sequential posterior estimation by using num_rounds>1 . This can decrease the number of simulations one has to run, but the inference procedure is no longer amortized. specify your own density estimator, or change hyperparameters of existing ones (e.g. number of hidden units for \u2018NSF\u2019). run simulations in batches, which can speed up simulations. if available, choose between different methods to sample from the posterior. use calibration kernels as proposed by Lueckmann, Goncalves et al. 2017 Linear Gaussian example \u00b6 import torch from sbi.inference import SNPE , prepare_for_sbi from sbi.utils.get_nn_models import posterior_nn import sbi.utils as utils We will show an example of how we can use the flexible interface to infer the posterior for an example with a Gaussian likelihood (same example as before). Let us first import some libraries and define the simulator and prior: num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 In the advanced mode, you have to ensure that your simulator and prior adhere the requirements of sbi . You can do so with the prepare_for_sbi() function. In addition prepare_for_sbi() returns the x_shape , which is the shape of a single simulation output. simulator , prior , x_shape = prepare_for_sbi ( linear_gaussian , prior ) You can then use the prior and x_shape object to specify a custom density estimator. Since we use \u2018SNPE\u2019, we specifiy a neural network targeting the posterior (hence the call to posterior_nn() ). In this example, we will create a neural spline flow ( 'nsf' ) with 60 hidden units and 3 transform layers: my_density_estimator = posterior_nn ( 'nsf' , prior , x_shape , hidden_features = 60 , flow_num_transforms = 3 ) We will set use SNPE with a simulation_batch_size=10 , i.e. 10 simulations will be passed to the simulator which will then handle the simulations in a vectorized way (note that your simulator has to support this in order to use this feature): inference = SNPE ( simulator , prior , x_shape , density_estimator = my_density_estimator , show_progress_bars = False ) And we can run inference. In this example, we will run inference over 2 rounds, potentially leading to a more focused posterior around the observation x_o . x_o = torch . zeros ( 3 ,) posterior = inference ( num_rounds = 2 , x_o = x_o , num_simulations_per_round = 1000 ) Note that, for num_rounds>1 , the posterior is no longer amortized: it will give good results when sampled around x=observation , but bad possibly bad results for other x . Once we have obtained the posterior, we can .sample() , .log_prob() , or .pairplot() in the same way as for the easy interface. posterior_samples = posterior . sample (( 10000 ,), x = x_o ) # plot posterior samples _ = utils . pairplot ( posterior_samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 ))","title":"Flexible interface"},{"location":"tutorial/markdown_files/03_flexible_interface/#the-flexible-interface","text":"In the previous tutorial, we have demonstrated how sbi can be used to run simulation-based inference with just a single line of code. In addition to this simple interface, sbi also provides a flexible interface which unlocks several additional features implemented in sbi .","title":"The flexible interface"},{"location":"tutorial/markdown_files/03_flexible_interface/#features","text":"The flexible interface allows you to customize the following: performing sequential posterior estimation by using num_rounds>1 . This can decrease the number of simulations one has to run, but the inference procedure is no longer amortized. specify your own density estimator, or change hyperparameters of existing ones (e.g. number of hidden units for \u2018NSF\u2019). run simulations in batches, which can speed up simulations. if available, choose between different methods to sample from the posterior. use calibration kernels as proposed by Lueckmann, Goncalves et al. 2017","title":"Features"},{"location":"tutorial/markdown_files/03_flexible_interface/#linear-gaussian-example","text":"import torch from sbi.inference import SNPE , prepare_for_sbi from sbi.utils.get_nn_models import posterior_nn import sbi.utils as utils We will show an example of how we can use the flexible interface to infer the posterior for an example with a Gaussian likelihood (same example as before). Let us first import some libraries and define the simulator and prior: num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 In the advanced mode, you have to ensure that your simulator and prior adhere the requirements of sbi . You can do so with the prepare_for_sbi() function. In addition prepare_for_sbi() returns the x_shape , which is the shape of a single simulation output. simulator , prior , x_shape = prepare_for_sbi ( linear_gaussian , prior ) You can then use the prior and x_shape object to specify a custom density estimator. Since we use \u2018SNPE\u2019, we specifiy a neural network targeting the posterior (hence the call to posterior_nn() ). In this example, we will create a neural spline flow ( 'nsf' ) with 60 hidden units and 3 transform layers: my_density_estimator = posterior_nn ( 'nsf' , prior , x_shape , hidden_features = 60 , flow_num_transforms = 3 ) We will set use SNPE with a simulation_batch_size=10 , i.e. 10 simulations will be passed to the simulator which will then handle the simulations in a vectorized way (note that your simulator has to support this in order to use this feature): inference = SNPE ( simulator , prior , x_shape , density_estimator = my_density_estimator , show_progress_bars = False ) And we can run inference. In this example, we will run inference over 2 rounds, potentially leading to a more focused posterior around the observation x_o . x_o = torch . zeros ( 3 ,) posterior = inference ( num_rounds = 2 , x_o = x_o , num_simulations_per_round = 1000 ) Note that, for num_rounds>1 , the posterior is no longer amortized: it will give good results when sampled around x=observation , but bad possibly bad results for other x . Once we have obtained the posterior, we can .sample() , .log_prob() , or .pairplot() in the same way as for the easy interface. posterior_samples = posterior . sample (( 10000 ,), x = x_o ) # plot posterior samples _ = utils . pairplot ( posterior_samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 ))","title":"Linear Gaussian example"},{"location":"tutorial/notebooks/00_getting_started/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Getting started with sbi \u00b6 import torch import sbi.utils as utils from sbi.inference.base import infer Running the inference procedure \u00b6 sbi provides a simple interface to run state-of-the-art algorithms for simulation-based inference. For inference, you need to provide two things: 1) a prior distribution that allows to sample parameter sets. 2) a simulator that takes parameter sets and produces simulation outputs. For example, we can have a 3-dimensional parameter space with a uniform prior between [-1,1] and a simulator that adds 1.0 and some Gaussian noise to the parameter set: num_dim = 3 prior = utils . BoxUniform ( - 2 * torch . ones ( num_dim ), 2 * torch . ones ( num_dim )) def simulator ( parameter_set ): return 1.0 + parameter_set + torch . randn ( parameter_set . shape ) * 0.1 sbi can then run inference: posterior = infer ( simulator , prior , 'SNPE' , num_simulations = 1000 ) Let\u2019s say we have made some observation \\(x\\) : observation = torch . zeros ( 3 ) Given this observation, we can then sample from the posterior \\(p(\\theta|x)\\) , evaluate its log-probability, or plot it. samples = posterior . sample (( 10000 ,), x = observation ) log_probability = posterior . log_prob ( samples , x = observation ) _ = utils . pairplot ( samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 6 , 6 )) Requirements for the simulator, prior, and observation \u00b6 For all algorithms, all you need to provide are a prior and a simulator. Let\u2019s talk about what requirements they need to satisfy. Prior \u00b6 A distribution that allows to sample parameter sets. Any datatype for the prior is allowed as long as it allows to call prior.sample() and prior.log_prob() . Simulator \u00b6 A python callable that takes in a parameter set and outputs data with some (even if very small) stochasticity. Allowed data types and shapes for input and output: - the input parameter set and the output have to be either a np.ndarray or a torch.Tensor . - the input parameter set should have either shape (1,N) or (N) , and the output must have shape (1,M) or (M) . Observation \u00b6 An observation \\(x_o\\) for which you want to infer the posterior \\(p(\\theta|x_o)\\) . Allowed data types and shapes: - either a np.ndarray or a torch.Tensor . - shape must be either (1,N) or (N) . Running different algorithms \u00b6 sbi implements three classes of algorithms that can be used to obtain the posterior distribution: SNPE, SNL, and SRE. You can try the different algorithms by simply swapping out the method : posterior = infer ( simulator , prior , 'SNPE' , num_simulations = 1000 ) posterior = infer ( simulator , prior , 'SNLE' , num_simulations = 1000 ) posterior = infer ( simulator , prior , 'SNRE' , num_simulations = 1000 ) You can then infer, sample, evaluate, and plot the posterior as described above.","title":"00 getting started"},{"location":"tutorial/notebooks/00_getting_started/#getting-started-with-sbi","text":"import torch import sbi.utils as utils from sbi.inference.base import infer","title":"Getting started with sbi"},{"location":"tutorial/notebooks/00_getting_started/#running-the-inference-procedure","text":"sbi provides a simple interface to run state-of-the-art algorithms for simulation-based inference. For inference, you need to provide two things: 1) a prior distribution that allows to sample parameter sets. 2) a simulator that takes parameter sets and produces simulation outputs. For example, we can have a 3-dimensional parameter space with a uniform prior between [-1,1] and a simulator that adds 1.0 and some Gaussian noise to the parameter set: num_dim = 3 prior = utils . BoxUniform ( - 2 * torch . ones ( num_dim ), 2 * torch . ones ( num_dim )) def simulator ( parameter_set ): return 1.0 + parameter_set + torch . randn ( parameter_set . shape ) * 0.1 sbi can then run inference: posterior = infer ( simulator , prior , 'SNPE' , num_simulations = 1000 ) Let\u2019s say we have made some observation \\(x\\) : observation = torch . zeros ( 3 ) Given this observation, we can then sample from the posterior \\(p(\\theta|x)\\) , evaluate its log-probability, or plot it. samples = posterior . sample (( 10000 ,), x = observation ) log_probability = posterior . log_prob ( samples , x = observation ) _ = utils . pairplot ( samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 6 , 6 ))","title":"Running the inference procedure"},{"location":"tutorial/notebooks/00_getting_started/#requirements-for-the-simulator-prior-and-observation","text":"For all algorithms, all you need to provide are a prior and a simulator. Let\u2019s talk about what requirements they need to satisfy.","title":"Requirements for the simulator, prior, and observation"},{"location":"tutorial/notebooks/00_getting_started/#prior","text":"A distribution that allows to sample parameter sets. Any datatype for the prior is allowed as long as it allows to call prior.sample() and prior.log_prob() .","title":"Prior"},{"location":"tutorial/notebooks/00_getting_started/#simulator","text":"A python callable that takes in a parameter set and outputs data with some (even if very small) stochasticity. Allowed data types and shapes for input and output: - the input parameter set and the output have to be either a np.ndarray or a torch.Tensor . - the input parameter set should have either shape (1,N) or (N) , and the output must have shape (1,M) or (M) .","title":"Simulator"},{"location":"tutorial/notebooks/00_getting_started/#observation","text":"An observation \\(x_o\\) for which you want to infer the posterior \\(p(\\theta|x_o)\\) . Allowed data types and shapes: - either a np.ndarray or a torch.Tensor . - shape must be either (1,N) or (N) .","title":"Observation"},{"location":"tutorial/notebooks/00_getting_started/#running-different-algorithms","text":"sbi implements three classes of algorithms that can be used to obtain the posterior distribution: SNPE, SNL, and SRE. You can try the different algorithms by simply swapping out the method : posterior = infer ( simulator , prior , 'SNPE' , num_simulations = 1000 ) posterior = infer ( simulator , prior , 'SNLE' , num_simulations = 1000 ) posterior = infer ( simulator , prior , 'SNRE' , num_simulations = 1000 ) You can then infer, sample, evaluate, and plot the posterior as described above.","title":"Running different algorithms"},{"location":"tutorial/notebooks/01_gaussian_amortized/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Amortized posterior inference on Gaussian example \u00b6 In this tutorial, we will demonstrate how sbi can infer an amortized posterior for a simple toy model with a uniform prior and Gaussian likelihood. import torch import numpy as np import sbi.utils as utils from sbi.inference.base import infer Defining prior, simulator, and running inference \u00b6 Say we have 3-dimensional parameter space, and the prior is uniformly distributed between -2 and 2 in each dimension. num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) Our simulator takes the input parameters, adds 1.0 in each dimension, and then adds some Gaussian noise: def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 We can then run inference: posterior = infer ( linear_gaussian , prior , 'SNPE' , num_simulations = 1000 ) Amortized inference \u00b6 As it can be seen above, we have not yet provided an observation to the inference procedure. In fact, we can evaluate the posterior for different observations without having to re-run inference. This is called amortization. Let\u2019s say we have two observations x_o_1 = [0,0,0] and x_o_2 = [2,2,2] : x_o_1 = torch . zeros ( 3 ,) x_o_2 = 2.0 * torch . ones ( 3 ,) We can draw samples from the posterior given x_o_1 and then plot them: posterior_samples_1 = posterior . sample (( 10000 ,), x = x_o_1 ) # plot posterior samples _ = utils . pairplot ( posterior_samples_1 , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 )) As it can be seen, the posterior samples are centered around [-1,-1,-1] in each dimension. This makes sense because the simulator always adds 1.0 in each dimension and we have observed x_o_1 = [0,0,0] . Since the obtained posterior is amortized, we can also draw samples from the posterior given the second observation without having to re-run interence: posterior_samples_2 = posterior . sample (( 10000 ,), x = x_o_2 ) # plot posterior samples _ = utils . pairplot ( posterior_samples_2 , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 )) So, if we have observed x_o_2 = [2,2,2] , the posterior is centered around [1,1,1] \u2013 again, this makes sense because the simulator adds 1.0 in each dimension.","title":"01 gaussian amortized"},{"location":"tutorial/notebooks/01_gaussian_amortized/#amortized-posterior-inference-on-gaussian-example","text":"In this tutorial, we will demonstrate how sbi can infer an amortized posterior for a simple toy model with a uniform prior and Gaussian likelihood. import torch import numpy as np import sbi.utils as utils from sbi.inference.base import infer","title":"Amortized posterior inference on Gaussian example"},{"location":"tutorial/notebooks/01_gaussian_amortized/#defining-prior-simulator-and-running-inference","text":"Say we have 3-dimensional parameter space, and the prior is uniformly distributed between -2 and 2 in each dimension. num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) Our simulator takes the input parameters, adds 1.0 in each dimension, and then adds some Gaussian noise: def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 We can then run inference: posterior = infer ( linear_gaussian , prior , 'SNPE' , num_simulations = 1000 )","title":"Defining prior, simulator, and running inference"},{"location":"tutorial/notebooks/01_gaussian_amortized/#amortized-inference","text":"As it can be seen above, we have not yet provided an observation to the inference procedure. In fact, we can evaluate the posterior for different observations without having to re-run inference. This is called amortization. Let\u2019s say we have two observations x_o_1 = [0,0,0] and x_o_2 = [2,2,2] : x_o_1 = torch . zeros ( 3 ,) x_o_2 = 2.0 * torch . ones ( 3 ,) We can draw samples from the posterior given x_o_1 and then plot them: posterior_samples_1 = posterior . sample (( 10000 ,), x = x_o_1 ) # plot posterior samples _ = utils . pairplot ( posterior_samples_1 , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 )) As it can be seen, the posterior samples are centered around [-1,-1,-1] in each dimension. This makes sense because the simulator always adds 1.0 in each dimension and we have observed x_o_1 = [0,0,0] . Since the obtained posterior is amortized, we can also draw samples from the posterior given the second observation without having to re-run interence: posterior_samples_2 = posterior . sample (( 10000 ,), x = x_o_2 ) # plot posterior samples _ = utils . pairplot ( posterior_samples_2 , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 )) So, if we have observed x_o_2 = [2,2,2] , the posterior is centered around [1,1,1] \u2013 again, this makes sense because the simulator adds 1.0 in each dimension.","title":"Amortized inference"},{"location":"tutorial/notebooks/02_HH_simulator/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Inference on Hodgkin-Huxley model: tutorial \u00b6 In this tutorial, we use sbi to do inference on a Hodgkin-Huxley model (Hodgkin and Huxley, 1952) with two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given a current-clamp recording (synthetically generated). First we are going to import basic packages. import numpy as np import torch # VISUALIZATION import matplotlib as mpl import matplotlib.pyplot as plt # sbi import sbi import sbi.utils as utils from sbi.inference.base import infer # remove top and right axis from plots mpl . rcParams [ 'axes.spines.right' ] = False mpl . rcParams [ 'axes.spines.top' ] = False Different required components \u00b6 Before running inference, let us define the different required components: observed data prior over model parameters simulator 1) Observed data \u00b6 Let us assume we current-clamped a neuron and recorded the following voltage trace: In fact, this voltage trace was not measured experimentally but synthetically generated by simulating a Hodgkin-Huxley model with particular parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). We will come back to this point later in the tutorial. 2) Simulator \u00b6 We would like to infer the posterior over the two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ) of a Hodgkin-Huxley model, given the observed electrophysiological recording above. The model has channel kinetics as in Pospischil et al. 2008, and is defined by the following set of differential equations (parameters of interest highlighted in orange): $$ \\scriptsize \\begin{align} C_m\\frac{dV}{dt}&=g_1\\left(E_1-V\\right)+ \\color{orange}{\\bar{g} {Na}}m^3h\\left(E -V\\right)+ \\color{orange}{\\bar{g} {K}}n^4\\left(E_K-V\\right)+ \\bar{g}_Mp\\left(E_K-V\\right)+ I + \\sigma\\eta\\left(t\\right)\\ \\frac{dq}{dt}&=\\frac{q_\\infty\\left(V\\right)-q}{\\tau_q\\left(V\\right)},\\;q\\in{m,h,n,p} \\end{align} $$ where \\(V\\) is the membrane potential, \\(C_m\\) is the membrane capacitance, \\(g_{\\text{l}}\\) is the leak conductance, \\(E_{\\text{l}}\\) is the membrane reversal potential, \\(\\bar{g}_c\\) is the density of channels of type \\(c\\) ( \\(\\text{Na}^+\\) , \\(\\text{K}^+\\) , M), \\(E_c\\) is the reversal potential of \\(c\\) , ( \\(m\\) , \\(h\\) , \\(n\\) , \\(p\\) ) are the respective channel gating kinetic variables, and \\(\\sigma \\eta(t)\\) is the intrinsic neural noise. The right hand side of the voltage dynamics is composed of a leak current, a voltage-dependent \\(\\text{Na}^+\\) current, a delayed-rectifier \\(\\text{K}^+\\) current, a slow voltage-dependent \\(\\text{K}^+\\) current responsible for spike-frequency adaptation, and an injected current \\(I_{\\text{inj}}\\) . Channel gating variables \\(q\\) have dynamics fully characterized by the neuron membrane potential \\(V\\) , given the respective steady-state \\(q_{\\infty}(V)\\) and time constant \\(\\tau_{q}(V)\\) (details in Pospischil et al. 2008). The input current \\(I_{\\text{inj}}\\) is defined as from HH_helper_functions import syn_current I , t_on , t_off , dt , t , A_soma = syn_current () The Hodgkin-Huxley simulator is given by: from HH_helper_functions import HHsimulator Putting the input current and the simulator together: def run_HH_model ( params ): # input current, time step I , t_on , t_off , dt , t , A_soma = syn_current () t = np . arange ( 0 , len ( I ), 1 ) * dt # initial voltage V0 = - 70 params = np . asarray ( params ) states = HHsimulator ( V0 , params . reshape ( 1 , - 1 ), dt , t , I , seed = 0 ) return { 'data' : states . reshape ( - 1 ), 'time' : t , 'dt' : dt , 'I' : I . reshape ( - 1 )} To get an idea of the output of the Hodgkin-Huxley model, let us generate some voltage traces for different parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the input current \\(I_{\\text{inj}}\\) : # three sets of (g_Na, g_K) params = np . array ([[ 50. , 1. ],[ 4. , 1.5 ],[ 20. , 15. ]]) num_samples = len ( params [:, 0 ]) sim_samples = np . zeros (( num_samples , len ( I ))) for i in range ( num_samples ): sim_samples [ i ,:] = run_HH_model ( params = params [ i ,:])[ 'data' ] # colors for traces col_min = 2 num_colors = num_samples + col_min cm1 = mpl . cm . Blues col1 = [ cm1 ( 1. * i / num_colors ) for i in range ( col_min , num_colors )] fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) for i in range ( num_samples ): plt . plot ( t , sim_samples [ i ,:], color = col1 [ i ], lw = 2 ) plt . ylabel ( 'voltage (mV)' ) ax . set_xticks ([]) ax . set_yticks ([ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( t , I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( t ) / 2 , max ( t )]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' )) As can be seen, the voltage traces can be quite diverse for different parameter values. Often, we are not interested in matching the exact trace, but only in matching certain features thereof. In this example of the Hodgkin Huxley model, the summary features are the number of spikes, the mean resting potential, the standard deviation of the resting potential, and the first 4 voltage moments, mean, standard deviation, skewness and kurtosis. In the function calculate_summary_statistics() below, we compute these statistics from the output of the Hodgkin Huxley simulator. from HH_helper_functions import calculate_summary_statistics Lastly, we define a function that performs all of the above steps at once. The function simulation_wrapper takes in conductance values, runs the Hodgkin Huxley model and then returns the summary statistics. def simulation_wrapper ( params ): \"\"\" Takes in conductance values and then first runs the Hodgkin Huxley model and then returns the summary statistics as torch.Tensor \"\"\" obs = run_HH_model ( params ) summstats = torch . as_tensor ( calculate_summary_statistics ( obs )) return summstats sbi takes any function as simulator. Thus, sbi also has the flexibility to use simulators that utilize external packages, e.g., Brian ( http://briansimulator.org/ ), nest ( https://www.nest-simulator.org/ ), or NEURON ( https://neuron.yale.edu/neuron/ ). External simulators do not even need to be Python-based as long as they store simulation outputs in a format that can be read from Python. All that is necessary is to wrap your external simulator of choice into a python callable that takes a parameter set and outputs a set of summary statistics we want to fit the parameters to. 3) Prior over model parameters \u00b6 Now that we have the simulator, we need to define a function with the prior over the model parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), which in this case is chosen to be a Uniform distribution: prior_min = [ . 5 , 1e-4 ] prior_max = [ 80. , 15. ] prior = utils . torchutils . BoxUniform ( low = torch . as_tensor ( prior_min ), high = torch . as_tensor ( prior_max )) Inference \u00b6 Now that we have all the required components, we can run inference with SNPE. We start by importing our SNPE object of choice. We will now perform inference with SNPE to identify parameters whose activity matches this trace: posterior = infer ( simulation_wrapper , prior , 'SNPE' , num_simulations = 300 , num_workers = 4 ) Coming back to the observed data \u00b6 As mentioned at the beginning of the tutorial, the observed data are generated by the Hodgkin-Huxley model with a set of known parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). To illustrate how to compute the summary statistics of the observed data, let us regenerate the observed data: # true parameters and respective labels true_params = np . array ([ 50. , 5. ]) labels_params = [ r '$g_ {Na} $' , r '$g_ {K} $' ] observation_trace = run_HH_model ( true_params ) observation_summary_statistics = calculate_summary_statistics ( observation_trace ) As we had already shown above, the observed voltage traces look as follows: fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) plt . plot ( observation_trace [ 'time' ], observation_trace [ 'data' ]) plt . ylabel ( 'voltage (mV)' ) plt . title ( 'observed data' ) ax . set_xticks ([]) ax . set_yticks ([ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( observation_trace [ 'time' ], I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( observation_trace [ 'time' ]) / 2 , max ( observation_trace [ 'time' ])]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' )) Analysis of the posterior given the observed data \u00b6 After running the inference algorithm, let us inspect the inferred posterior distribution over the parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the observed trace. To do so, we first draw samples (i.e. consistent parameter sets) from the posterior: samples = posterior . sample (( 10000 ,), x = observation_summary_statistics ) fig , axes = utils . pairplot ( samples , limits = [[ . 5 , 80 ], [ 1e-4 , 15. ]], ticks = [[ . 5 , 80 ], [ 1e-4 , 15. ]], fig_size = ( 5 , 5 ), points = true_params , points_offdiag = { 'markersize' : 10 }, points_colors = 'r' ); As can be seen, the inferred posterior contains the ground-truth parameters (red) in a high-probability region. Now, let us sample parameters from the posterior distribution, simulate the Hodgkin-Huxley model for this parameter set and compare the simulations with the observed data: # draw a sample from the posterior and convert to numpy posterior_sample = posterior . sample ( num_samples = 1 , x = observation_summary_statistics ) . numpy () fig = plt . figure ( figsize = ( 7 , 5 )) # plot observation t = observation_trace [ 'time' ] y_obs = observation_trace [ 'data' ] plt . plot ( t , y_obs , lw = 2 , label = 'observation' ) # simulate and plot samples x = run_HH_model ( posterior_sample ) plt . plot ( t , x [ 'data' ], '--' , lw = 2 , label = 'posterior sample' ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'voltage (mV)' ) ax = plt . gca () handles , labels = ax . get_legend_handles_labels () ax . legend ( handles [:: - 1 ], labels [:: - 1 ], bbox_to_anchor = ( 1.3 , 1 ), loc = 'upper right' ) ax . set_xticks ([ 0 , 60 , 120 ]) ax . set_yticks ([ - 80 , - 20 , 40 ]); As can be seen, the sample from the inferred posterior leads to simulations that closely resemble the observed data, confirming that SNPE did a good job at capturing the observed data. References \u00b6 A. L. Hodgkin and A. F. Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. The Journal of Physiology, 117(4):500\u2013544, 1952. M. Pospischil, M. Toledo-Rodriguez, C. Monier, Z. Piwkowska, T. Bal, Y. Fr\u00e9gnac, H. Markram, and A. Destexhe. Minimal Hodgkin-Huxley type models for different classes of cortical and thalamic neurons. Biological Cybernetics, 99(4-5), 2008.","title":"02 HH simulator"},{"location":"tutorial/notebooks/02_HH_simulator/#inference-on-hodgkin-huxley-model-tutorial","text":"In this tutorial, we use sbi to do inference on a Hodgkin-Huxley model (Hodgkin and Huxley, 1952) with two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given a current-clamp recording (synthetically generated). First we are going to import basic packages. import numpy as np import torch # VISUALIZATION import matplotlib as mpl import matplotlib.pyplot as plt # sbi import sbi import sbi.utils as utils from sbi.inference.base import infer # remove top and right axis from plots mpl . rcParams [ 'axes.spines.right' ] = False mpl . rcParams [ 'axes.spines.top' ] = False","title":"Inference on Hodgkin-Huxley model: tutorial"},{"location":"tutorial/notebooks/02_HH_simulator/#different-required-components","text":"Before running inference, let us define the different required components: observed data prior over model parameters simulator","title":"Different required components"},{"location":"tutorial/notebooks/02_HH_simulator/#1-observed-data","text":"Let us assume we current-clamped a neuron and recorded the following voltage trace: In fact, this voltage trace was not measured experimentally but synthetically generated by simulating a Hodgkin-Huxley model with particular parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). We will come back to this point later in the tutorial.","title":"1) Observed data"},{"location":"tutorial/notebooks/02_HH_simulator/#2-simulator","text":"We would like to infer the posterior over the two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ) of a Hodgkin-Huxley model, given the observed electrophysiological recording above. The model has channel kinetics as in Pospischil et al. 2008, and is defined by the following set of differential equations (parameters of interest highlighted in orange): $$ \\scriptsize \\begin{align} C_m\\frac{dV}{dt}&=g_1\\left(E_1-V\\right)+ \\color{orange}{\\bar{g} {Na}}m^3h\\left(E -V\\right)+ \\color{orange}{\\bar{g} {K}}n^4\\left(E_K-V\\right)+ \\bar{g}_Mp\\left(E_K-V\\right)+ I + \\sigma\\eta\\left(t\\right)\\ \\frac{dq}{dt}&=\\frac{q_\\infty\\left(V\\right)-q}{\\tau_q\\left(V\\right)},\\;q\\in{m,h,n,p} \\end{align} $$ where \\(V\\) is the membrane potential, \\(C_m\\) is the membrane capacitance, \\(g_{\\text{l}}\\) is the leak conductance, \\(E_{\\text{l}}\\) is the membrane reversal potential, \\(\\bar{g}_c\\) is the density of channels of type \\(c\\) ( \\(\\text{Na}^+\\) , \\(\\text{K}^+\\) , M), \\(E_c\\) is the reversal potential of \\(c\\) , ( \\(m\\) , \\(h\\) , \\(n\\) , \\(p\\) ) are the respective channel gating kinetic variables, and \\(\\sigma \\eta(t)\\) is the intrinsic neural noise. The right hand side of the voltage dynamics is composed of a leak current, a voltage-dependent \\(\\text{Na}^+\\) current, a delayed-rectifier \\(\\text{K}^+\\) current, a slow voltage-dependent \\(\\text{K}^+\\) current responsible for spike-frequency adaptation, and an injected current \\(I_{\\text{inj}}\\) . Channel gating variables \\(q\\) have dynamics fully characterized by the neuron membrane potential \\(V\\) , given the respective steady-state \\(q_{\\infty}(V)\\) and time constant \\(\\tau_{q}(V)\\) (details in Pospischil et al. 2008). The input current \\(I_{\\text{inj}}\\) is defined as from HH_helper_functions import syn_current I , t_on , t_off , dt , t , A_soma = syn_current () The Hodgkin-Huxley simulator is given by: from HH_helper_functions import HHsimulator Putting the input current and the simulator together: def run_HH_model ( params ): # input current, time step I , t_on , t_off , dt , t , A_soma = syn_current () t = np . arange ( 0 , len ( I ), 1 ) * dt # initial voltage V0 = - 70 params = np . asarray ( params ) states = HHsimulator ( V0 , params . reshape ( 1 , - 1 ), dt , t , I , seed = 0 ) return { 'data' : states . reshape ( - 1 ), 'time' : t , 'dt' : dt , 'I' : I . reshape ( - 1 )} To get an idea of the output of the Hodgkin-Huxley model, let us generate some voltage traces for different parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the input current \\(I_{\\text{inj}}\\) : # three sets of (g_Na, g_K) params = np . array ([[ 50. , 1. ],[ 4. , 1.5 ],[ 20. , 15. ]]) num_samples = len ( params [:, 0 ]) sim_samples = np . zeros (( num_samples , len ( I ))) for i in range ( num_samples ): sim_samples [ i ,:] = run_HH_model ( params = params [ i ,:])[ 'data' ] # colors for traces col_min = 2 num_colors = num_samples + col_min cm1 = mpl . cm . Blues col1 = [ cm1 ( 1. * i / num_colors ) for i in range ( col_min , num_colors )] fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) for i in range ( num_samples ): plt . plot ( t , sim_samples [ i ,:], color = col1 [ i ], lw = 2 ) plt . ylabel ( 'voltage (mV)' ) ax . set_xticks ([]) ax . set_yticks ([ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( t , I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( t ) / 2 , max ( t )]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' )) As can be seen, the voltage traces can be quite diverse for different parameter values. Often, we are not interested in matching the exact trace, but only in matching certain features thereof. In this example of the Hodgkin Huxley model, the summary features are the number of spikes, the mean resting potential, the standard deviation of the resting potential, and the first 4 voltage moments, mean, standard deviation, skewness and kurtosis. In the function calculate_summary_statistics() below, we compute these statistics from the output of the Hodgkin Huxley simulator. from HH_helper_functions import calculate_summary_statistics Lastly, we define a function that performs all of the above steps at once. The function simulation_wrapper takes in conductance values, runs the Hodgkin Huxley model and then returns the summary statistics. def simulation_wrapper ( params ): \"\"\" Takes in conductance values and then first runs the Hodgkin Huxley model and then returns the summary statistics as torch.Tensor \"\"\" obs = run_HH_model ( params ) summstats = torch . as_tensor ( calculate_summary_statistics ( obs )) return summstats sbi takes any function as simulator. Thus, sbi also has the flexibility to use simulators that utilize external packages, e.g., Brian ( http://briansimulator.org/ ), nest ( https://www.nest-simulator.org/ ), or NEURON ( https://neuron.yale.edu/neuron/ ). External simulators do not even need to be Python-based as long as they store simulation outputs in a format that can be read from Python. All that is necessary is to wrap your external simulator of choice into a python callable that takes a parameter set and outputs a set of summary statistics we want to fit the parameters to.","title":"2) Simulator"},{"location":"tutorial/notebooks/02_HH_simulator/#3-prior-over-model-parameters","text":"Now that we have the simulator, we need to define a function with the prior over the model parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), which in this case is chosen to be a Uniform distribution: prior_min = [ . 5 , 1e-4 ] prior_max = [ 80. , 15. ] prior = utils . torchutils . BoxUniform ( low = torch . as_tensor ( prior_min ), high = torch . as_tensor ( prior_max ))","title":"3) Prior over model parameters"},{"location":"tutorial/notebooks/02_HH_simulator/#inference","text":"Now that we have all the required components, we can run inference with SNPE. We start by importing our SNPE object of choice. We will now perform inference with SNPE to identify parameters whose activity matches this trace: posterior = infer ( simulation_wrapper , prior , 'SNPE' , num_simulations = 300 , num_workers = 4 )","title":"Inference"},{"location":"tutorial/notebooks/02_HH_simulator/#coming-back-to-the-observed-data","text":"As mentioned at the beginning of the tutorial, the observed data are generated by the Hodgkin-Huxley model with a set of known parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). To illustrate how to compute the summary statistics of the observed data, let us regenerate the observed data: # true parameters and respective labels true_params = np . array ([ 50. , 5. ]) labels_params = [ r '$g_ {Na} $' , r '$g_ {K} $' ] observation_trace = run_HH_model ( true_params ) observation_summary_statistics = calculate_summary_statistics ( observation_trace ) As we had already shown above, the observed voltage traces look as follows: fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) plt . plot ( observation_trace [ 'time' ], observation_trace [ 'data' ]) plt . ylabel ( 'voltage (mV)' ) plt . title ( 'observed data' ) ax . set_xticks ([]) ax . set_yticks ([ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( observation_trace [ 'time' ], I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( observation_trace [ 'time' ]) / 2 , max ( observation_trace [ 'time' ])]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' ))","title":"Coming back to the observed data"},{"location":"tutorial/notebooks/02_HH_simulator/#analysis-of-the-posterior-given-the-observed-data","text":"After running the inference algorithm, let us inspect the inferred posterior distribution over the parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the observed trace. To do so, we first draw samples (i.e. consistent parameter sets) from the posterior: samples = posterior . sample (( 10000 ,), x = observation_summary_statistics ) fig , axes = utils . pairplot ( samples , limits = [[ . 5 , 80 ], [ 1e-4 , 15. ]], ticks = [[ . 5 , 80 ], [ 1e-4 , 15. ]], fig_size = ( 5 , 5 ), points = true_params , points_offdiag = { 'markersize' : 10 }, points_colors = 'r' ); As can be seen, the inferred posterior contains the ground-truth parameters (red) in a high-probability region. Now, let us sample parameters from the posterior distribution, simulate the Hodgkin-Huxley model for this parameter set and compare the simulations with the observed data: # draw a sample from the posterior and convert to numpy posterior_sample = posterior . sample ( num_samples = 1 , x = observation_summary_statistics ) . numpy () fig = plt . figure ( figsize = ( 7 , 5 )) # plot observation t = observation_trace [ 'time' ] y_obs = observation_trace [ 'data' ] plt . plot ( t , y_obs , lw = 2 , label = 'observation' ) # simulate and plot samples x = run_HH_model ( posterior_sample ) plt . plot ( t , x [ 'data' ], '--' , lw = 2 , label = 'posterior sample' ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'voltage (mV)' ) ax = plt . gca () handles , labels = ax . get_legend_handles_labels () ax . legend ( handles [:: - 1 ], labels [:: - 1 ], bbox_to_anchor = ( 1.3 , 1 ), loc = 'upper right' ) ax . set_xticks ([ 0 , 60 , 120 ]) ax . set_yticks ([ - 80 , - 20 , 40 ]); As can be seen, the sample from the inferred posterior leads to simulations that closely resemble the observed data, confirming that SNPE did a good job at capturing the observed data.","title":"Analysis of the posterior given the observed data"},{"location":"tutorial/notebooks/02_HH_simulator/#references","text":"A. L. Hodgkin and A. F. Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. The Journal of Physiology, 117(4):500\u2013544, 1952. M. Pospischil, M. Toledo-Rodriguez, C. Monier, Z. Piwkowska, T. Bal, Y. Fr\u00e9gnac, H. Markram, and A. Destexhe. Minimal Hodgkin-Huxley type models for different classes of cortical and thalamic neurons. Biological Cybernetics, 99(4-5), 2008.","title":"References"},{"location":"tutorial/notebooks/03_flexible_interface/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); The flexible interface \u00b6 In the previous tutorial, we have demonstrated how sbi can be used to run simulation-based inference with just a single line of code. In addition to this simple interface, sbi also provides a flexible interface which unlocks several additional features implemented in sbi . Features \u00b6 The flexible interface allows you to customize the following: performing sequential posterior estimation by using num_rounds>1 . This can decrease the number of simulations one has to run, but the inference procedure is no longer amortized. specify your own density estimator, or change hyperparameters of existing ones (e.g. number of hidden units for \u2018NSF\u2019). run simulations in batches, which can speed up simulations. if available, choose between different methods to sample from the posterior. use calibration kernels as proposed by Lueckmann, Goncalves et al. 2017 Linear Gaussian example \u00b6 import torch from sbi.inference import SNPE , prepare_for_sbi from sbi.utils.get_nn_models import posterior_nn import sbi.utils as utils We will show an example of how we can use the flexible interface to infer the posterior for an example with a Gaussian likelihood (same example as before). Let us first import some libraries and define the simulator and prior: num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 In the advanced mode, you have to ensure that your simulator and prior adhere the requirements of sbi . You can do so with the prepare_for_sbi() function. In addition prepare_for_sbi() returns the x_shape , which is the shape of a single simulation output. simulator , prior , x_shape = prepare_for_sbi ( linear_gaussian , prior ) You can then use the prior and x_shape object to specify a custom density estimator. Since we use \u2018SNPE\u2019, we specifiy a neural network targeting the posterior (hence the call to posterior_nn() ). In this example, we will create a neural spline flow ( 'nsf' ) with 60 hidden units and 3 transform layers: my_density_estimator = posterior_nn ( 'nsf' , prior , x_shape , hidden_features = 60 , flow_num_transforms = 3 ) We will set use SNPE with a simulation_batch_size=10 , i.e. 10 simulations will be passed to the simulator which will then handle the simulations in a vectorized way (note that your simulator has to support this in order to use this feature): inference = SNPE ( simulator , prior , x_shape , density_estimator = my_density_estimator , show_progress_bars = False ) And we can run inference. In this example, we will run inference over 2 rounds, potentially leading to a more focused posterior around the observation x_o . x_o = torch . zeros ( 3 ,) posterior = inference ( num_rounds = 2 , x_o = x_o , num_simulations_per_round = 1000 ) Note that, for num_rounds>1 , the posterior is no longer amortized: it will give good results when sampled around x=observation , but bad possibly bad results for other x . Once we have obtained the posterior, we can .sample() , .log_prob() , or .pairplot() in the same way as for the easy interface. posterior_samples = posterior . sample (( 10000 ,), x = x_o ) # plot posterior samples _ = utils . pairplot ( posterior_samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 ))","title":"03 flexible interface"},{"location":"tutorial/notebooks/03_flexible_interface/#the-flexible-interface","text":"In the previous tutorial, we have demonstrated how sbi can be used to run simulation-based inference with just a single line of code. In addition to this simple interface, sbi also provides a flexible interface which unlocks several additional features implemented in sbi .","title":"The flexible interface"},{"location":"tutorial/notebooks/03_flexible_interface/#features","text":"The flexible interface allows you to customize the following: performing sequential posterior estimation by using num_rounds>1 . This can decrease the number of simulations one has to run, but the inference procedure is no longer amortized. specify your own density estimator, or change hyperparameters of existing ones (e.g. number of hidden units for \u2018NSF\u2019). run simulations in batches, which can speed up simulations. if available, choose between different methods to sample from the posterior. use calibration kernels as proposed by Lueckmann, Goncalves et al. 2017","title":"Features"},{"location":"tutorial/notebooks/03_flexible_interface/#linear-gaussian-example","text":"import torch from sbi.inference import SNPE , prepare_for_sbi from sbi.utils.get_nn_models import posterior_nn import sbi.utils as utils We will show an example of how we can use the flexible interface to infer the posterior for an example with a Gaussian likelihood (same example as before). Let us first import some libraries and define the simulator and prior: num_dim = 3 prior = utils . BoxUniform ( low =- 2 * torch . ones ( num_dim ), high = 2 * torch . ones ( num_dim )) def linear_gaussian ( theta ): return theta + 1.0 + torch . randn_like ( theta ) * 0.1 In the advanced mode, you have to ensure that your simulator and prior adhere the requirements of sbi . You can do so with the prepare_for_sbi() function. In addition prepare_for_sbi() returns the x_shape , which is the shape of a single simulation output. simulator , prior , x_shape = prepare_for_sbi ( linear_gaussian , prior ) You can then use the prior and x_shape object to specify a custom density estimator. Since we use \u2018SNPE\u2019, we specifiy a neural network targeting the posterior (hence the call to posterior_nn() ). In this example, we will create a neural spline flow ( 'nsf' ) with 60 hidden units and 3 transform layers: my_density_estimator = posterior_nn ( 'nsf' , prior , x_shape , hidden_features = 60 , flow_num_transforms = 3 ) We will set use SNPE with a simulation_batch_size=10 , i.e. 10 simulations will be passed to the simulator which will then handle the simulations in a vectorized way (note that your simulator has to support this in order to use this feature): inference = SNPE ( simulator , prior , x_shape , density_estimator = my_density_estimator , show_progress_bars = False ) And we can run inference. In this example, we will run inference over 2 rounds, potentially leading to a more focused posterior around the observation x_o . x_o = torch . zeros ( 3 ,) posterior = inference ( num_rounds = 2 , x_o = x_o , num_simulations_per_round = 1000 ) Note that, for num_rounds>1 , the posterior is no longer amortized: it will give good results when sampled around x=observation , but bad possibly bad results for other x . Once we have obtained the posterior, we can .sample() , .log_prob() , or .pairplot() in the same way as for the easy interface. posterior_samples = posterior . sample (( 10000 ,), x = x_o ) # plot posterior samples _ = utils . pairplot ( posterior_samples , limits = [[ - 2 , 2 ],[ - 2 , 2 ],[ - 2 , 2 ]], fig_size = ( 5 , 5 ))","title":"Linear Gaussian example"}]}