{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"sbi \u00b6 sbi : A Python toolbox to perform simulation-based inference using density-estimation approaches. The focus of sbi is Sequential Neural Posterior Estimation (SNPE). In SNPE, a neural network is trained to perform Bayesian inference on simulated data. To see illustrations of SNPE on canonical problems in neuroscience, read our preprint: Training deep neural density estimators to identify mechanistic models of neural dynamics . To learn more about the general motivation behind simulation-based inference, and inference methods included in sbi , keep on reading. Motivation and approach \u00b6 Many areas of science and engineering make extensive use of complex, stochastic, numerical simulations to describe the structure and dynamics of the processes being investigated. A key challenge in simulation-based science is linking simulation models to empirical data: Bayesian inference provides a general and powerful framework for identifying the set of parameters which are consistent both with empirical data and prior knowledge. One of the key quantities required for statistical inference, the likelihood of observed data given parameters, \\(\\mathcal{L}(\\theta) = p(x_o|\\theta)\\) , is typically intractable for simulation-based models, rendering conventional statistical approaches inapplicable. Sequential Neural Posterior Estimation (SNPE) is a powerful machine-learning technique to address this problem. Goal: Algorithmically identify mechanistic models which are consistent with data. SNPE takes three inputs: A candidate mechanistic model, prior knowledge or constraints on model parameters, and data (or summary statistics). SNPE proceeds by: sampling parameters from the prior and simulating synthetic datasets from these parameters, and using a deep density estimation neural network to learn the (probabilistic) association between data (or data features) and underlying parameters, i.e. to learn statistical inference from simulated data. This density estimation network is then applied to empirical data to derive the full space of parameters consistent with the data and the prior, i.e. the posterior distribution. High posterior probability is assigned to parameters which are consistent with both the data and the prior, low probability to inconsistent parameters. If needed, an initial estimate of the posterior can be used to adaptively generate additional informative simulations. Publications \u00b6 Refer to the following papers for additional details on the inference methods included in sbi : Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios & Murray (NeurIPS 2016) [PDF] [BibTeX] Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) [PDF] [BibTeX] Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher & Macke (ICML 2019) [PDF] [BibTeX] On Contrastive Learning for Likelihood-free Inference Durkan C., Murray I., and Papamakarios G.(ICML 2020) [PDF] . We refer to these methods as SNPE-A, SNPE-B, and SNPE-C/APT, respectively. As an alternative to directly estimating the posterior on parameters given data, it is also possible to estimate the likelihood of data given parameters, and then subsequently draw posterior samples using MCMC ( Papamakarios, Sterratt & Murray, 2019 1 , Lueckmann, Karaletsos, Bassetto, Macke, 2019 ). Depending on the problem, approximating the likelihood can be more or less effective than SNPE techniques. See Cranmer, Brehmer, Louppe (2019) for a recent review on simulation-based inference and our recent preprint Training deep neural density estimators to identify mechanistic models of neural dynamics (Goncalves et al., 2019) for applications to canonical problems in neuroscience. Code for SNL is available from the original repository or as a python 3 package . \u21a9","title":"Home"},{"location":"#sbi","text":"sbi : A Python toolbox to perform simulation-based inference using density-estimation approaches. The focus of sbi is Sequential Neural Posterior Estimation (SNPE). In SNPE, a neural network is trained to perform Bayesian inference on simulated data. To see illustrations of SNPE on canonical problems in neuroscience, read our preprint: Training deep neural density estimators to identify mechanistic models of neural dynamics . To learn more about the general motivation behind simulation-based inference, and inference methods included in sbi , keep on reading.","title":"sbi"},{"location":"#motivation-and-approach","text":"Many areas of science and engineering make extensive use of complex, stochastic, numerical simulations to describe the structure and dynamics of the processes being investigated. A key challenge in simulation-based science is linking simulation models to empirical data: Bayesian inference provides a general and powerful framework for identifying the set of parameters which are consistent both with empirical data and prior knowledge. One of the key quantities required for statistical inference, the likelihood of observed data given parameters, \\(\\mathcal{L}(\\theta) = p(x_o|\\theta)\\) , is typically intractable for simulation-based models, rendering conventional statistical approaches inapplicable. Sequential Neural Posterior Estimation (SNPE) is a powerful machine-learning technique to address this problem. Goal: Algorithmically identify mechanistic models which are consistent with data. SNPE takes three inputs: A candidate mechanistic model, prior knowledge or constraints on model parameters, and data (or summary statistics). SNPE proceeds by: sampling parameters from the prior and simulating synthetic datasets from these parameters, and using a deep density estimation neural network to learn the (probabilistic) association between data (or data features) and underlying parameters, i.e. to learn statistical inference from simulated data. This density estimation network is then applied to empirical data to derive the full space of parameters consistent with the data and the prior, i.e. the posterior distribution. High posterior probability is assigned to parameters which are consistent with both the data and the prior, low probability to inconsistent parameters. If needed, an initial estimate of the posterior can be used to adaptively generate additional informative simulations.","title":"Motivation and approach"},{"location":"#publications","text":"Refer to the following papers for additional details on the inference methods included in sbi : Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios & Murray (NeurIPS 2016) [PDF] [BibTeX] Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) [PDF] [BibTeX] Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher & Macke (ICML 2019) [PDF] [BibTeX] On Contrastive Learning for Likelihood-free Inference Durkan C., Murray I., and Papamakarios G.(ICML 2020) [PDF] . We refer to these methods as SNPE-A, SNPE-B, and SNPE-C/APT, respectively. As an alternative to directly estimating the posterior on parameters given data, it is also possible to estimate the likelihood of data given parameters, and then subsequently draw posterior samples using MCMC ( Papamakarios, Sterratt & Murray, 2019 1 , Lueckmann, Karaletsos, Bassetto, Macke, 2019 ). Depending on the problem, approximating the likelihood can be more or less effective than SNPE techniques. See Cranmer, Brehmer, Louppe (2019) for a recent review on simulation-based inference and our recent preprint Training deep neural density estimators to identify mechanistic models of neural dynamics (Goncalves et al., 2019) for applications to canonical problems in neuroscience. Code for SNL is available from the original repository or as a python 3 package . \u21a9","title":"Publications"},{"location":"contribute/","text":"Issues \u00b6 Open issues on GitHub for any problems you encounter. Pull requests \u00b6 In general, we use pull requests to make changes to sbi . Code style \u00b6 For docstrings and comments, we use Google Style . Code needs to pass through the following tools, which are installed along with sbi : black : Automatic code formatting for Python. You can run black manually from the console using black . in the top directory of the repository, which will format all files. isort : Used to consistently order imports. You can run isort manually from the console using isort -y in the top directory. Documentation \u00b6 The documentation is entirely written in markdown ( basic markdown guide ). It would be of great help, if you fixed mistakes, and edited where things are unclear. After a PR with documentation changes has been merged, the online documentation will be updated automatically in a couple of minutes. If you want to test a local build of the documentation, take a look at docs/README.md .","title":"Contribute"},{"location":"contribute/#issues","text":"Open issues on GitHub for any problems you encounter.","title":"Issues"},{"location":"contribute/#pull-requests","text":"In general, we use pull requests to make changes to sbi .","title":"Pull requests"},{"location":"contribute/#code-style","text":"For docstrings and comments, we use Google Style . Code needs to pass through the following tools, which are installed along with sbi : black : Automatic code formatting for Python. You can run black manually from the console using black . in the top directory of the repository, which will format all files. isort : Used to consistently order imports. You can run isort manually from the console using isort -y in the top directory.","title":"Code style"},{"location":"contribute/#documentation","text":"The documentation is entirely written in markdown ( basic markdown guide ). It would be of great help, if you fixed mistakes, and edited where things are unclear. After a PR with documentation changes has been merged, the online documentation will be updated automatically in a couple of minutes. If you want to test a local build of the documentation, take a look at docs/README.md .","title":"Documentation"},{"location":"credits/","text":"Credits \u00b6 sbi is licensed under the Affero General Public License version 3 (AGPLv3) and Copyright (C) 2020 Michael Deistler, Jan F. B\u00f6lts, Jan-Matthis L\u00fcckmann, \u00c1lvaro Tejero-Cantero. Copyright (C) 2020 Conor M. Durkan. Code \u00b6 sbi uses density estimators from bayesiains/nflows by Conor M.Durkan , George Papamakarios and Artur Bekasov . sbi started as a fork of conormdurkan/lfi , by [Conor M.Durkan]( https://conormdurkan.github.io/ . Inference methods \u00b6 sbi implements inference methods reported in the following contributions: Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios G. and Murray I. (NeurIPS 2016) [PDF] [BibTeX] . Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann J-M., Gon\u00e7alves P., Bassetto G., \u00d6cal K., Nonnenmacher M. and Macke J. (NeurIPS 2017) [PDF] [BibTeX] . Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher M. and Macke J. (ICML 2019) [PDF] [BibTeX] . On Contrastive Learning for Likelihood-free Inference Durkan C., Murray I., and Papamakarios G.(ICML 2020) [PDF] . We refer to these methods as SNPE-A, SNPE-B, and SNPE-C/APT, respectively.","title":"Credits"},{"location":"credits/#credits","text":"sbi is licensed under the Affero General Public License version 3 (AGPLv3) and Copyright (C) 2020 Michael Deistler, Jan F. B\u00f6lts, Jan-Matthis L\u00fcckmann, \u00c1lvaro Tejero-Cantero. Copyright (C) 2020 Conor M. Durkan.","title":"Credits"},{"location":"credits/#code","text":"sbi uses density estimators from bayesiains/nflows by Conor M.Durkan , George Papamakarios and Artur Bekasov . sbi started as a fork of conormdurkan/lfi , by [Conor M.Durkan]( https://conormdurkan.github.io/ .","title":"Code"},{"location":"credits/#inference-methods","text":"sbi implements inference methods reported in the following contributions: Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios G. and Murray I. (NeurIPS 2016) [PDF] [BibTeX] . Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann J-M., Gon\u00e7alves P., Bassetto G., \u00d6cal K., Nonnenmacher M. and Macke J. (NeurIPS 2017) [PDF] [BibTeX] . Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher M. and Macke J. (ICML 2019) [PDF] [BibTeX] . On Contrastive Learning for Likelihood-free Inference Durkan C., Murray I., and Papamakarios G.(ICML 2020) [PDF] . We refer to these methods as SNPE-A, SNPE-B, and SNPE-C/APT, respectively.","title":"Inference methods"},{"location":"install/","text":"Installation \u00b6 Quick start \u00b6 Clone the repo and install all the dependencies using the environment.yml file to create a conda environment: conda env create -f environment.yml . If you already have an sbi environment and want to refresh dependencies, just run conda env update -f environment.yml --prune . Alternatively, you can install via setup.py using pip install -e \".[dev]\" (the dev flag installs development and testing dependencies). git clone https://github.com/mackelab/sbi.git cd sbi pip install -e \".dev\"","title":"Installation"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#quick-start","text":"Clone the repo and install all the dependencies using the environment.yml file to create a conda environment: conda env create -f environment.yml . If you already have an sbi environment and want to refresh dependencies, just run conda env update -f environment.yml --prune . Alternatively, you can install via setup.py using pip install -e \".[dev]\" (the dev flag installs development and testing dependencies). git clone https://github.com/mackelab/sbi.git cd sbi pip install -e \".dev\"","title":"Quick start"},{"location":"reference/","text":"API Reference \u00b6 Inference \u00b6 sbi.inference.snpe.snpe_a.SNPE_A \u00b6 __init__ ( self , simulator , prior , x_shape = None , num_workers = 1 , simulation_batch_size = 1 , density_estimator = 'mdn' , calibration_kernel = None , z_score_x = True , z_score_min_std = 1e-07 , exclude_invalid_x = True , device = device ( type = 'cpu' ), logging_level = 'WARNING' , summary_writer = None , show_progress_bars = True , show_round_summary = False ) special SNPE-A [1]. CURRENTLY NOT IMPLEMENTED. [1] Fast epsilon-free Inference of Simulation Models with Bayesian Conditional Density Estimation , Papamakarios et al., NeurIPS 2016, https://arxiv.org/abs/1605.06376 . Source code in sbi/inference/snpe/snpe_a.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , simulator : Callable , prior , x_shape : Optional [ torch . Size ] = None , num_workers : int = 1 , simulation_batch_size : int = 1 , density_estimator : Union [ str , nn . Module ] = \"mdn\" , calibration_kernel : Optional [ Callable ] = None , z_score_x : bool = True , z_score_min_std : float = 1e-7 , exclude_invalid_x : bool = True , device : Union [ torch . device , str ] = get_default_device (), logging_level : Union [ int , str ] = \"WARNING\" , summary_writer : Optional [ SummaryWriter ] = None , show_progress_bars : bool = True , show_round_summary : bool = False , ): \"\"\"SNPE-A [1]. CURRENTLY NOT IMPLEMENTED. [1] _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional Density Estimation_, Papamakarios et al., NeurIPS 2016, https://arxiv.org/abs/1605.06376. \"\"\" raise NotImplementedError sbi.inference.snpe.snpe_b.SNPE_B \u00b6 __init__ ( self , simulator , prior , x_shape = None , num_workers = 1 , simulation_batch_size = 1 , density_estimator = 'mdn' , z_score_x = True , z_score_min_std = 1e-07 , calibration_kernel = None , retrain_from_scratch_each_round = False , discard_prior_samples = False , exclude_invalid_x = True , device = device ( type = 'cpu' ), logging_level = 'WARNING' , summary_writer = None , show_progress_bars = True , show_round_summary = False ) special SNPE-B [1]. CURRENTLY NOT IMPLEMENTED. [1] Flexible statistical inference for mechanistic models of neural dynamics , Lueckmann, Gon\u00e7alves et al., NeurIPS 2017, https://arxiv.org/abs/1711.01861 . See docstring of PosteriorEstimator class for all other arguments. Source code in sbi/inference/snpe/snpe_b.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __init__ ( self , simulator : Callable , prior , x_shape : Optional [ torch . Size ] = None , num_workers : int = 1 , simulation_batch_size : Optional [ int ] = 1 , density_estimator : Union [ str , nn . Module ] = \"mdn\" , z_score_x : bool = True , z_score_min_std : float = 1e-7 , calibration_kernel : Optional [ Callable ] = None , retrain_from_scratch_each_round : bool = False , discard_prior_samples : bool = False , exclude_invalid_x : bool = True , device : Union [ torch . device , str ] = get_default_device (), logging_level : Union [ int , str ] = \"WARNING\" , summary_writer : Optional [ SummaryWriter ] = None , show_progress_bars : bool = True , show_round_summary : bool = False , ): r \"\"\"SNPE-B [1]. CURRENTLY NOT IMPLEMENTED. [1] _Flexible statistical inference for mechanistic models of neural dynamics_, Lueckmann, Gon\u00e7alves et al., NeurIPS 2017, https://arxiv.org/abs/1711.01861. See docstring of `PosteriorEstimator` class for all other arguments. \"\"\" raise NotImplementedError ( \"SNPE-B is not yet implemented in the sbi package, see issue #199.\" ) super () . __init__ ( simulator = simulator , prior = prior , x_shape = x_shape , num_workers = num_workers , simulation_batch_size = simulation_batch_size , density_estimator = density_estimator , z_score_x = z_score_x , z_score_min_std = z_score_min_std , calibration_kernel = calibration_kernel , retrain_from_scratch_each_round = retrain_from_scratch_each_round , discard_prior_samples = discard_prior_samples , exclude_invalid_x = exclude_invalid_x , device = device , logging_level = logging_level , show_progress_bars = show_progress_bars , show_round_summary = show_round_summary , ) sbi.inference.snpe.snpe_c.SNPE_C \u00b6 __call__ ( self , num_rounds , num_simulations_per_round , x_o = None , num_atoms = 10 , batch_size = 100 , learning_rate = 0.0005 , validation_fraction = 0.1 , stop_after_epochs = 20 , max_num_epochs = None , clip_max_norm = 5.0 , calibration_kernel = None , exclude_invalid_x = True , z_score_x = True , z_score_min_std = 1e-07 , discard_prior_samples = False , retrain_from_scratch_each_round = False ) special Parameters: Name Type Description Default num_atoms int Number of atoms to use for classification. 10 Returns: Type Description NeuralPosterior Source code in sbi/inference/snpe/snpe_c.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __call__ ( self , num_rounds : int , num_simulations_per_round : OneOrMore [ int ], x_o : Optional [ Tensor ] = None , num_atoms : int = 10 , batch_size : int = 100 , learning_rate : float = 5e-4 , validation_fraction : float = 0.1 , stop_after_epochs : int = 20 , max_num_epochs : Optional [ int ] = None , clip_max_norm : Optional [ float ] = 5.0 , calibration_kernel : Optional [ Callable ] = None , exclude_invalid_x : bool = True , z_score_x : bool = True , z_score_min_std : float = 1e-7 , discard_prior_samples : bool = False , retrain_from_scratch_each_round : bool = False , ) -> NeuralPosterior : \"\"\" Args: num_atoms: Number of atoms to use for classification. Returns: \"\"\" # WARNING: sneaky trick ahead. We proxy the parent's `__call__` here, # requiring the signature to have `num_atoms`, save it for use below, and # continue. It's sneaky because we are using the object (self) as a namespace # to pass arguments between functions, and that's implicit state management. self . _num_atoms = num_atoms kwargs = del_entries ( locals (), entries = ( \"self\" , \"__class__\" , \"num_atoms\" )) return super () . __call__ ( ** kwargs ) __init__ ( self , simulator , prior , x_shape = None , num_workers = 1 , simulation_batch_size = 1 , density_estimator = 'maf' , sample_with_mcmc = False , mcmc_method = 'slice_np' , use_combined_loss = False , device = device ( type = 'cpu' ), logging_level = 'WARNING' , summary_writer = None , show_progress_bars = True , show_round_summary = False ) special SNPE-C / APT [1]. [1] Automatic Posterior Transformation for Likelihood-free Inference , Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488 . Parameters: Name Type Description Default use_combined_loss bool Whether to train the neural net also on prior samples using maximum likelihood in addition to training it on all samples using atomic loss. The extra MLE loss helps prevent density leaking with bounded priors. False See docstring of PosteriorEstimator class for all other arguments. Source code in sbi/inference/snpe/snpe_c.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , simulator : Callable , prior , x_shape : Optional [ torch . Size ] = None , num_workers : int = 1 , simulation_batch_size : int = 1 , density_estimator : Union [ str , nn . Module ] = \"maf\" , sample_with_mcmc : bool = False , mcmc_method : str = \"slice_np\" , use_combined_loss : bool = False , device : Union [ torch . device , str ] = get_default_device (), logging_level : Union [ int , str ] = \"WARNING\" , summary_writer : Optional [ SummaryWriter ] = None , show_progress_bars : bool = True , show_round_summary : bool = False , ): r \"\"\"SNPE-C / APT [1]. [1] _Automatic Posterior Transformation for Likelihood-free Inference_, Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488. Args: use_combined_loss: Whether to train the neural net also on prior samples using maximum likelihood in addition to training it on all samples using atomic loss. The extra MLE loss helps prevent density leaking with bounded priors. See docstring of `PosteriorEstimator` class for all other arguments. \"\"\" self . _use_combined_loss = use_combined_loss super () . __init__ ( simulator = simulator , prior = prior , x_shape = x_shape , num_workers = num_workers , simulation_batch_size = simulation_batch_size , density_estimator = density_estimator , sample_with_mcmc = sample_with_mcmc , mcmc_method = mcmc_method , device = device , logging_level = logging_level , summary_writer = summary_writer , show_progress_bars = show_progress_bars , show_round_summary = show_round_summary , ) sbi.inference.snre.snre_a.SNRE_A \u00b6 AALR[1], here known as SNRE_A. [1] _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_, Hermans et al., ICML 2020, https://arxiv.org/abs/1903.04057 Models \u00b6 sbi.utils.get_nn_models.posterior_nn ( model , prior_mean , prior_std , x_o_shape , embedding = Identity (), hidden_features = 50 , mdn_num_components = 20 , made_num_mixture_components = 10 , made_num_blocks = 4 , flow_num_transforms = 5 ) \u00b6 Neural posterior density estimator Parameters: Name Type Description Default model str Model, one of maf / mdn / made / nsf required prior_mean Tensor Prior mean. required prior_std Tensor Prior standard deviation. required x_o_numel Number of elements in the a single observation. Used as input size to the NN. required embedding nn.Module Embedding network Identity() hidden_features int For all, number of hidden features 50 mdn_num_components int For MDNs only, number of components 20 made_num_mixture_components int For MADEs only, number of mixture components 10 made_num_blocks int For MADEs only, number of blocks 4 flow_num_transforms int For flows only, number of transforms 5 Returns: Type Description nn.Module Neural network Source code in sbi/utils/get_nn_models.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def posterior_nn ( model : str , prior_mean : Tensor , prior_std : Tensor , x_o_shape : torch . Size , embedding : nn . Module = nn . Identity (), hidden_features : int = 50 , mdn_num_components : int = 20 , made_num_mixture_components : int = 10 , made_num_blocks : int = 4 , flow_num_transforms : int = 5 , ) -> nn . Module : \"\"\"Neural posterior density estimator Args: model: Model, one of maf / mdn / made / nsf prior_mean: Prior mean. prior_std: Prior standard deviation. x_o_numel: Number of elements in the a single observation. Used as input size to the NN. embedding: Embedding network hidden_features: For all, number of hidden features mdn_num_components: For MDNs only, number of components made_num_mixture_components: For MADEs only, number of mixture components made_num_blocks: For MADEs only, number of blocks flow_num_transforms: For flows only, number of transforms Returns: Neural network \"\"\" # We need these asserts because mean and std can be defined outside, prior to user # input checks. assert ( prior_mean . dtype == float32 ), f \"Prior mean must have dtype float32, is { prior_mean . dtype } .\" assert ( prior_std . dtype == float32 ), f \"Prior std must have dtype float32, is { prior_std . dtype } .\" standardizing_transform = transforms . AffineTransform ( shift =- prior_mean / prior_std , scale = 1 / prior_std ) theta_numel = prior_mean . numel () x_o_numel = x_o_shape . numel () if theta_numel == 1 : _check_1d_flow_limitations ( model , \"parameter\" ) if model == \"mdn\" : neural_net = MultivariateGaussianMDN ( features = theta_numel , context_features = x_o_numel , hidden_features = hidden_features , hidden_net = nn . Sequential ( nn . Linear ( x_o_numel , hidden_features ), nn . ReLU (), nn . Dropout ( p = 0.0 ), nn . Linear ( hidden_features , hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . ReLU (), ), num_components = mdn_num_components , custom_initialization = True , ) elif model == \"made\" : transform = standardizing_transform distribution = distributions_ . MADEMoG ( features = theta_numel , hidden_features = hidden_features , context_features = x_o_numel , num_blocks = made_num_blocks , num_mixture_components = made_num_mixture_components , use_residual_blocks = True , random_mask = False , activation = relu , dropout_probability = 0.0 , use_batch_norm = False , custom_initialization = True , ) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"maf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . MaskedAffineAutoregressiveTransform ( features = theta_numel , hidden_features = hidden_features , context_features = x_o_numel , num_blocks = 2 , use_residual_blocks = False , random_mask = False , activation = tanh , dropout_probability = 0.0 , use_batch_norm = True , ), transforms . RandomPermutation ( features = theta_numel ), ] ) for _ in range ( flow_num_transforms ) ] ) transform = transforms . CompositeTransform ([ standardizing_transform , transform ,]) distribution = distributions_ . StandardNormal (( theta_numel ,)) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"nsf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . PiecewiseRationalQuadraticCouplingTransform ( mask = create_alternating_binary_mask ( features = theta_numel , even = ( i % 2 == 0 ) ), transform_net_create_fn = lambda in_features , out_features : nets . ResidualNet ( in_features = in_features , out_features = out_features , hidden_features = hidden_features , context_features = x_o_numel , num_blocks = 2 , activation = relu , dropout_probability = 0.0 , use_batch_norm = False , ), num_bins = 10 , tails = \"linear\" , tail_bound = 3.0 , apply_unconditional_transform = False , ), transforms . LULinear ( theta_numel , identity_init = True ), ] ) for i in range ( flow_num_transforms ) ] ) transform = transforms . CompositeTransform ([ standardizing_transform , transform ,]) distribution = distributions_ . StandardNormal (( theta_numel ,)) neural_net = flows . Flow ( transform , distribution , embedding ) else : raise ValueError return neural_net sbi.utils.get_nn_models.likelihood_nn ( model , theta_shape , x_o_shape , embedding = None , hidden_features = 50 , mdn_num_components = 20 , made_num_mixture_components = 10 , made_num_blocks = 4 , flow_num_transforms = 5 ) \u00b6 Neural likelihood density estimator Parameters: Name Type Description Default model str Model, one of maf / mdn / made / nsf required theta_numel event shape of the prior, number of parameters. required x_o_numel number of elements in a single data point. required embedding Optional[nn.Module] Embedding network None hidden_features int For all, number of hidden features 50 mdn_num_components int For MDNs only, number of components 20 made_num_mixture_components int For MADEs only, number of mixture components 10 made_num_blocks int For MADEs only, number of blocks 4 flow_num_transforms int For flows only, number of transforms 5 Returns: Type Description nn.Module Neural network Source code in sbi/utils/get_nn_models.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 def likelihood_nn ( model : str , theta_shape : torch . Size , x_o_shape : torch . Size , embedding : Optional [ nn . Module ] = None , hidden_features : int = 50 , mdn_num_components : int = 20 , made_num_mixture_components : int = 10 , made_num_blocks : int = 4 , flow_num_transforms : int = 5 , ) -> nn . Module : \"\"\"Neural likelihood density estimator Args: model: Model, one of maf / mdn / made / nsf theta_numel: event shape of the prior, number of parameters. x_o_numel: number of elements in a single data point. embedding: Embedding network hidden_features: For all, number of hidden features mdn_num_components: For MDNs only, number of components made_num_mixture_components: For MADEs only, number of mixture components made_num_blocks: For MADEs only, number of blocks flow_num_transforms: For flows only, number of transforms Returns: Neural network \"\"\" theta_numel = theta_shape . numel () x_o_numel = x_o_shape . numel () if x_o_numel == 1 : _check_1d_flow_limitations ( model , \"data\" ) if model == \"mdn\" : neural_net = MultivariateGaussianMDN ( features = x_o_numel , context_features = theta_numel , hidden_features = hidden_features , hidden_net = nn . Sequential ( nn . Linear ( theta_numel , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Dropout ( p = 0.0 ), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), ), num_components = mdn_num_components , custom_initialization = True , ) elif model == \"made\" : neural_net = MixtureOfGaussiansMADE ( features = x_o_numel , hidden_features = hidden_features , context_features = theta_numel , num_blocks = made_num_blocks , num_mixture_components = made_num_mixture_components , use_residual_blocks = True , random_mask = False , activation = relu , use_batch_norm = True , dropout_probability = 0.0 , custom_initialization = True , ) elif model == \"maf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . MaskedAffineAutoregressiveTransform ( features = x_o_numel , hidden_features = hidden_features , context_features = theta_numel , num_blocks = 2 , use_residual_blocks = False , random_mask = False , activation = tanh , dropout_probability = 0.0 , use_batch_norm = True , ), transforms . RandomPermutation ( features = x_o_numel ), ] ) for _ in range ( flow_num_transforms ) ] ) distribution = distributions_ . StandardNormal (( x_o_numel ,)) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"nsf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . PiecewiseRationalQuadraticCouplingTransform ( mask = create_alternating_binary_mask ( features = x_o_numel , even = ( i % 2 == 0 ) ), transform_net_create_fn = lambda in_features , out_features : nets . ResidualNet ( in_features = in_features , out_features = out_features , hidden_features = hidden_features , context_features = theta_numel , num_blocks = 2 , activation = relu , dropout_probability = 0.0 , use_batch_norm = False , ), num_bins = 10 , tails = \"linear\" , tail_bound = 3.0 , apply_unconditional_transform = False , ), transforms . LULinear ( x_o_numel , identity_init = True ), ] ) for i in range ( flow_num_transforms ) ] ) distribution = distributions_ . StandardNormal (( x_o_numel ,)) neural_net = flows . Flow ( transform , distribution ) else : raise ValueError return neural_net sbi.utils.get_nn_models.classifier_nn ( model , theta_shape , x_o_shape , hidden_features = 50 ) \u00b6 Neural classifier Parameters: Name Type Description Default model Model, one of linear / mlp / resnet required theta_numel event shape of the prior, number of parameters. required x_o_numel number of elements in a single data point. required hidden_features int For all, number of hidden features 50 Returns: Type Description nn.Module Neural network Source code in sbi/utils/get_nn_models.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 def classifier_nn ( model , theta_shape : torch . Size , x_o_shape : torch . Size , hidden_features : int = 50 , ) -> nn . Module : \"\"\"Neural classifier Args: model: Model, one of linear / mlp / resnet theta_numel: event shape of the prior, number of parameters. x_o_numel: number of elements in a single data point. hidden_features: For all, number of hidden features Returns: Neural network \"\"\" theta_numel = theta_shape . numel () x_o_numel = x_o_shape . numel () if model == \"linear\" : neural_net = nn . Linear ( theta_numel + x_o_numel , 1 ) elif model == \"mlp\" : neural_net = nn . Sequential ( nn . Linear ( theta_numel + x_o_numel , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , 1 ), ) elif model == \"resnet\" : neural_net = nets . ResidualNet ( in_features = theta_numel + x_o_numel , out_features = 1 , hidden_features = hidden_features , context_features = None , num_blocks = 2 , activation = relu , dropout_probability = 0.0 , use_batch_norm = False , ) else : raise ValueError ( f \"'model' must be one of ['linear', 'mlp', 'resnet'].\" ) return neural_net","title":"API Reference"},{"location":"reference/#api-reference","text":"","title":"API Reference"},{"location":"reference/#inference","text":"","title":"Inference"},{"location":"reference/#sbi.inference.snpe.snpe_a.SNPE_A","text":"","title":"SNPE_A"},{"location":"reference/#sbi.inference.snpe.snpe_b.SNPE_B","text":"","title":"SNPE_B"},{"location":"reference/#sbi.inference.snpe.snpe_c.SNPE_C","text":"","title":"SNPE_C"},{"location":"reference/#sbi.inference.snre.snre_a.SNRE_A","text":"AALR[1], here known as SNRE_A. [1] _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_, Hermans et al., ICML 2020, https://arxiv.org/abs/1903.04057","title":"SNRE_A"},{"location":"reference/#models","text":"","title":"Models"},{"location":"reference/#sbi.utils.get_nn_models.posterior_nn","text":"Neural posterior density estimator Parameters: Name Type Description Default model str Model, one of maf / mdn / made / nsf required prior_mean Tensor Prior mean. required prior_std Tensor Prior standard deviation. required x_o_numel Number of elements in the a single observation. Used as input size to the NN. required embedding nn.Module Embedding network Identity() hidden_features int For all, number of hidden features 50 mdn_num_components int For MDNs only, number of components 20 made_num_mixture_components int For MADEs only, number of mixture components 10 made_num_blocks int For MADEs only, number of blocks 4 flow_num_transforms int For flows only, number of transforms 5 Returns: Type Description nn.Module Neural network Source code in sbi/utils/get_nn_models.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def posterior_nn ( model : str , prior_mean : Tensor , prior_std : Tensor , x_o_shape : torch . Size , embedding : nn . Module = nn . Identity (), hidden_features : int = 50 , mdn_num_components : int = 20 , made_num_mixture_components : int = 10 , made_num_blocks : int = 4 , flow_num_transforms : int = 5 , ) -> nn . Module : \"\"\"Neural posterior density estimator Args: model: Model, one of maf / mdn / made / nsf prior_mean: Prior mean. prior_std: Prior standard deviation. x_o_numel: Number of elements in the a single observation. Used as input size to the NN. embedding: Embedding network hidden_features: For all, number of hidden features mdn_num_components: For MDNs only, number of components made_num_mixture_components: For MADEs only, number of mixture components made_num_blocks: For MADEs only, number of blocks flow_num_transforms: For flows only, number of transforms Returns: Neural network \"\"\" # We need these asserts because mean and std can be defined outside, prior to user # input checks. assert ( prior_mean . dtype == float32 ), f \"Prior mean must have dtype float32, is { prior_mean . dtype } .\" assert ( prior_std . dtype == float32 ), f \"Prior std must have dtype float32, is { prior_std . dtype } .\" standardizing_transform = transforms . AffineTransform ( shift =- prior_mean / prior_std , scale = 1 / prior_std ) theta_numel = prior_mean . numel () x_o_numel = x_o_shape . numel () if theta_numel == 1 : _check_1d_flow_limitations ( model , \"parameter\" ) if model == \"mdn\" : neural_net = MultivariateGaussianMDN ( features = theta_numel , context_features = x_o_numel , hidden_features = hidden_features , hidden_net = nn . Sequential ( nn . Linear ( x_o_numel , hidden_features ), nn . ReLU (), nn . Dropout ( p = 0.0 ), nn . Linear ( hidden_features , hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . ReLU (), ), num_components = mdn_num_components , custom_initialization = True , ) elif model == \"made\" : transform = standardizing_transform distribution = distributions_ . MADEMoG ( features = theta_numel , hidden_features = hidden_features , context_features = x_o_numel , num_blocks = made_num_blocks , num_mixture_components = made_num_mixture_components , use_residual_blocks = True , random_mask = False , activation = relu , dropout_probability = 0.0 , use_batch_norm = False , custom_initialization = True , ) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"maf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . MaskedAffineAutoregressiveTransform ( features = theta_numel , hidden_features = hidden_features , context_features = x_o_numel , num_blocks = 2 , use_residual_blocks = False , random_mask = False , activation = tanh , dropout_probability = 0.0 , use_batch_norm = True , ), transforms . RandomPermutation ( features = theta_numel ), ] ) for _ in range ( flow_num_transforms ) ] ) transform = transforms . CompositeTransform ([ standardizing_transform , transform ,]) distribution = distributions_ . StandardNormal (( theta_numel ,)) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"nsf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . PiecewiseRationalQuadraticCouplingTransform ( mask = create_alternating_binary_mask ( features = theta_numel , even = ( i % 2 == 0 ) ), transform_net_create_fn = lambda in_features , out_features : nets . ResidualNet ( in_features = in_features , out_features = out_features , hidden_features = hidden_features , context_features = x_o_numel , num_blocks = 2 , activation = relu , dropout_probability = 0.0 , use_batch_norm = False , ), num_bins = 10 , tails = \"linear\" , tail_bound = 3.0 , apply_unconditional_transform = False , ), transforms . LULinear ( theta_numel , identity_init = True ), ] ) for i in range ( flow_num_transforms ) ] ) transform = transforms . CompositeTransform ([ standardizing_transform , transform ,]) distribution = distributions_ . StandardNormal (( theta_numel ,)) neural_net = flows . Flow ( transform , distribution , embedding ) else : raise ValueError return neural_net","title":"posterior_nn()"},{"location":"reference/#sbi.utils.get_nn_models.likelihood_nn","text":"Neural likelihood density estimator Parameters: Name Type Description Default model str Model, one of maf / mdn / made / nsf required theta_numel event shape of the prior, number of parameters. required x_o_numel number of elements in a single data point. required embedding Optional[nn.Module] Embedding network None hidden_features int For all, number of hidden features 50 mdn_num_components int For MDNs only, number of components 20 made_num_mixture_components int For MADEs only, number of mixture components 10 made_num_blocks int For MADEs only, number of blocks 4 flow_num_transforms int For flows only, number of transforms 5 Returns: Type Description nn.Module Neural network Source code in sbi/utils/get_nn_models.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 def likelihood_nn ( model : str , theta_shape : torch . Size , x_o_shape : torch . Size , embedding : Optional [ nn . Module ] = None , hidden_features : int = 50 , mdn_num_components : int = 20 , made_num_mixture_components : int = 10 , made_num_blocks : int = 4 , flow_num_transforms : int = 5 , ) -> nn . Module : \"\"\"Neural likelihood density estimator Args: model: Model, one of maf / mdn / made / nsf theta_numel: event shape of the prior, number of parameters. x_o_numel: number of elements in a single data point. embedding: Embedding network hidden_features: For all, number of hidden features mdn_num_components: For MDNs only, number of components made_num_mixture_components: For MADEs only, number of mixture components made_num_blocks: For MADEs only, number of blocks flow_num_transforms: For flows only, number of transforms Returns: Neural network \"\"\" theta_numel = theta_shape . numel () x_o_numel = x_o_shape . numel () if x_o_numel == 1 : _check_1d_flow_limitations ( model , \"data\" ) if model == \"mdn\" : neural_net = MultivariateGaussianMDN ( features = x_o_numel , context_features = theta_numel , hidden_features = hidden_features , hidden_net = nn . Sequential ( nn . Linear ( theta_numel , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Dropout ( p = 0.0 ), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), ), num_components = mdn_num_components , custom_initialization = True , ) elif model == \"made\" : neural_net = MixtureOfGaussiansMADE ( features = x_o_numel , hidden_features = hidden_features , context_features = theta_numel , num_blocks = made_num_blocks , num_mixture_components = made_num_mixture_components , use_residual_blocks = True , random_mask = False , activation = relu , use_batch_norm = True , dropout_probability = 0.0 , custom_initialization = True , ) elif model == \"maf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . MaskedAffineAutoregressiveTransform ( features = x_o_numel , hidden_features = hidden_features , context_features = theta_numel , num_blocks = 2 , use_residual_blocks = False , random_mask = False , activation = tanh , dropout_probability = 0.0 , use_batch_norm = True , ), transforms . RandomPermutation ( features = x_o_numel ), ] ) for _ in range ( flow_num_transforms ) ] ) distribution = distributions_ . StandardNormal (( x_o_numel ,)) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"nsf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . PiecewiseRationalQuadraticCouplingTransform ( mask = create_alternating_binary_mask ( features = x_o_numel , even = ( i % 2 == 0 ) ), transform_net_create_fn = lambda in_features , out_features : nets . ResidualNet ( in_features = in_features , out_features = out_features , hidden_features = hidden_features , context_features = theta_numel , num_blocks = 2 , activation = relu , dropout_probability = 0.0 , use_batch_norm = False , ), num_bins = 10 , tails = \"linear\" , tail_bound = 3.0 , apply_unconditional_transform = False , ), transforms . LULinear ( x_o_numel , identity_init = True ), ] ) for i in range ( flow_num_transforms ) ] ) distribution = distributions_ . StandardNormal (( x_o_numel ,)) neural_net = flows . Flow ( transform , distribution ) else : raise ValueError return neural_net","title":"likelihood_nn()"},{"location":"reference/#sbi.utils.get_nn_models.classifier_nn","text":"Neural classifier Parameters: Name Type Description Default model Model, one of linear / mlp / resnet required theta_numel event shape of the prior, number of parameters. required x_o_numel number of elements in a single data point. required hidden_features int For all, number of hidden features 50 Returns: Type Description nn.Module Neural network Source code in sbi/utils/get_nn_models.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 def classifier_nn ( model , theta_shape : torch . Size , x_o_shape : torch . Size , hidden_features : int = 50 , ) -> nn . Module : \"\"\"Neural classifier Args: model: Model, one of linear / mlp / resnet theta_numel: event shape of the prior, number of parameters. x_o_numel: number of elements in a single data point. hidden_features: For all, number of hidden features Returns: Neural network \"\"\" theta_numel = theta_shape . numel () x_o_numel = x_o_shape . numel () if model == \"linear\" : neural_net = nn . Linear ( theta_numel + x_o_numel , 1 ) elif model == \"mlp\" : neural_net = nn . Sequential ( nn . Linear ( theta_numel + x_o_numel , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , 1 ), ) elif model == \"resnet\" : neural_net = nets . ResidualNet ( in_features = theta_numel + x_o_numel , out_features = 1 , hidden_features = hidden_features , context_features = None , num_blocks = 2 , activation = relu , dropout_probability = 0.0 , use_batch_norm = False , ) else : raise ValueError ( f \"'model' must be one of ['linear', 'mlp', 'resnet'].\" ) return neural_net","title":"classifier_nn()"}]}